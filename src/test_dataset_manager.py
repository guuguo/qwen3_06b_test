#!/usr/bin/env python3
"""
æµ‹è¯•é›†ç®¡ç†å’ŒéªŒè¯å·¥å…·

æä¾›æµ‹è¯•é›†çš„åŠ è½½ã€éªŒè¯ã€ç®¡ç†å’Œè¯„ä¼°åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æµ‹è¯•é›†æ•°æ®åŠ è½½å’ŒéªŒè¯
- æ¨¡å‹è¯„ä¼°å’Œç»“æœåˆ†æ
- æµ‹è¯•é›†ç»Ÿè®¡å’ŒæŠ¥å‘Šç”Ÿæˆ
- é¢„è®¾æµ‹è¯•åœºæ™¯ç®¡ç†

ä½œè€…: Qwen-3 éƒ¨ç½²å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
"""

import json
import os
import logging
import random
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple, Union
from dataclasses import dataclass, asdict
from pathlib import Path
import statistics
import sys

# æ·»åŠ promptsç›®å½•åˆ°Pythonè·¯å¾„
current_dir = Path(__file__).parent.parent
prompts_dir = current_dir / "prompts"
sys.path.append(str(prompts_dir))

from md_manager import get_md_prompt_manager


@dataclass
class TestSample:
    """æµ‹è¯•æ ·æœ¬æ•°æ®ç±»"""
    id: str
    content: str
    category: str
    expected_score: float
    keywords: List[str]
    expected_response: str
    metadata: Dict[str, Any] = None


@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    sample_id: str
    comment: str  # æ·»åŠ åŸå§‹è¯„è®ºå†…å®¹
    model_response: str
    model_score: Optional[float]
    model_category: Optional[str]
    expected_score: float
    expected_category: str
    score_accuracy: float
    category_match: bool
    response_time_ms: float
    error: Optional[str] = None


@dataclass
class EvaluationReport:
    """è¯„ä¼°æŠ¥å‘Šæ•°æ®ç±»"""
    dataset_name: str
    model_name: str
    test_time: str
    total_samples: int
    successful_tests: int
    failed_tests: int
    avg_score_accuracy: float
    category_accuracy: float
    avg_response_time_ms: float
    score_distribution: Dict[str, int]
    category_distribution: Dict[str, int]
    detailed_results: List[TestResult]


class TestDatasetManager:
    """
    æµ‹è¯•é›†ç®¡ç†å™¨
    
    è´Ÿè´£åŠ è½½ã€éªŒè¯å’Œç®¡ç†å„ç§æµ‹è¯•é›†æ•°æ®
    """
    
    def __init__(self, datasets_dir: str = "./test_datasets"):
        """
        åˆå§‹åŒ–æµ‹è¯•é›†ç®¡ç†å™¨
        
        Args:
            datasets_dir: æµ‹è¯•é›†ç›®å½•è·¯å¾„
        """
        self.datasets_dir = Path(datasets_dir)
        self.logger = logging.getLogger(__name__)
        self.loaded_datasets = {}
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.datasets_dir.mkdir(exist_ok=True)
        
        self.logger.info(f"æµ‹è¯•é›†ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œç›®å½•: {datasets_dir}")
    
    def load_dataset(self, dataset_name: str) -> Optional[Dict[str, Any]]:
        """
        åŠ è½½æµ‹è¯•é›†æ•°æ®
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°ï¼ˆä¸å«æ‰©å±•åï¼‰
            
        Returns:
            Dict: æµ‹è¯•é›†æ•°æ®ï¼Œå¤±è´¥è¿”å› None
        """
        dataset_file = self.datasets_dir / f"{dataset_name}.json"
        
        if not dataset_file.exists():
            self.logger.error(f"æµ‹è¯•é›†æ–‡ä»¶ä¸å­˜åœ¨: {dataset_file}")
            return None
        
        try:
            with open(dataset_file, 'r', encoding='utf-8') as f:
                dataset = json.load(f)
            
            # éªŒè¯æ•°æ®é›†æ ¼å¼
            if not self._validate_dataset(dataset):
                self.logger.error(f"æµ‹è¯•é›†æ ¼å¼éªŒè¯å¤±è´¥: {dataset_name}")
                return None
            
            self.loaded_datasets[dataset_name] = dataset
            self.logger.info(f"æµ‹è¯•é›†åŠ è½½æˆåŠŸ: {dataset_name}, "
                           f"æ ·æœ¬æ•°: {len(dataset.get('test_samples', []))}")
            
            return dataset
            
        except Exception as e:
            self.logger.error(f"åŠ è½½æµ‹è¯•é›†å¤±è´¥: {dataset_name}, é”™è¯¯: {e}")
            return None
    
    def _validate_dataset(self, dataset: Dict[str, Any]) -> bool:
        """éªŒè¯æµ‹è¯•é›†æ•°æ®æ ¼å¼"""
        required_fields = ['dataset_info', 'test_samples']
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        for field in required_fields:
            if field not in dataset:
                self.logger.error(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False
        
        # æ£€æŸ¥æ•°æ®é›†ä¿¡æ¯
        info = dataset['dataset_info']
        required_info_fields = ['name', 'description', 'version', 'total_samples']
        for field in required_info_fields:
            if field not in info:
                self.logger.error(f"æ•°æ®é›†ä¿¡æ¯ç¼ºå°‘å­—æ®µ: {field}")
                return False
        
        # æ£€æŸ¥æ ·æœ¬æ•°é‡
        samples = dataset['test_samples']
        if len(samples) != info['total_samples']:
            self.logger.warning(f"æ ·æœ¬æ•°é‡ä¸åŒ¹é…: å£°æ˜{info['total_samples']}, å®é™…{len(samples)}")
        
        # æ£€æŸ¥æ ·æœ¬æ ¼å¼
        for i, sample in enumerate(samples):
            required_sample_fields = ['id']
            for field in required_sample_fields:
                if field not in sample:
                    self.logger.error(f"æ ·æœ¬ {i} ç¼ºå°‘å­—æ®µ: {field}")
                    return False
        
        return True
    
    def get_test_samples(self, dataset_name: str, 
                        sample_count: Optional[int] = None,
                        categories: Optional[List[str]] = None,
                        random_seed: Optional[int] = None) -> List[TestSample]:
        """
        è·å–æµ‹è¯•æ ·æœ¬
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°
            sample_count: æ ·æœ¬æ•°é‡é™åˆ¶ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
            categories: ç­›é€‰ç‰¹å®šç±»åˆ«ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨
            random_seed: éšæœºç§å­ï¼Œç”¨äºç¡®ä¿ç»“æœå¯é‡ç°
            
        Returns:
            List[TestSample]: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
        """
        if dataset_name not in self.loaded_datasets:
            dataset = self.load_dataset(dataset_name)
            if not dataset:
                return []
        else:
            dataset = self.loaded_datasets[dataset_name]
        
        samples = dataset['test_samples']
        test_samples = []
        
        # è½¬æ¢ä¸º TestSample å¯¹è±¡
        for sample in samples:
            # æ ¹æ®ä¸åŒæ•°æ®é›†ç±»å‹è§£ææ ·æœ¬
            if dataset_name == 'call_semantic_complaints':
                test_sample = TestSample(
                    id=sample['id'],
                    content=sample['conversation_text'],
                    category=sample['category'],
                    expected_score=sample['severity_score'],
                    keywords=sample['keywords'],
                    expected_response=sample['expected_response'],
                    metadata={
                        'severity_score': sample['severity_score'],
                        'keywords': sample['keywords']
                    }
                )
            elif dataset_name == 'manga_comment_ad_detection':
                test_sample = TestSample(
                    id=sample['id'],
                    content=sample['comment_text'],
                    category=sample['category'],
                    expected_score=sample['ad_score'],
                    keywords=sample['keywords'],
                    expected_response=sample['expected_response'],
                    metadata={
                        'ad_score': sample['ad_score'],
                        'context': sample['context'],
                        'category': sample['category']
                    }
                )
            else:
                # é€šç”¨æ ¼å¼
                test_sample = TestSample(
                    id=sample['id'],
                    content=sample.get('content', ''),
                    category=sample.get('category', ''),
                    expected_score=sample.get('expected_score', 0),
                    keywords=sample.get('keywords', []),
                    expected_response=sample.get('expected_response', ''),
                    metadata=sample.get('metadata', {})
                )
            
            test_samples.append(test_sample)
        
        # ç±»åˆ«ç­›é€‰
        if categories:
            test_samples = [s for s in test_samples if s.category in categories]
        
        # éšæœºé‡‡æ ·
        if sample_count and sample_count < len(test_samples):
            if random_seed is not None:
                random.seed(random_seed)
            test_samples = random.sample(test_samples, sample_count)
        
        self.logger.info(f"è·å–æµ‹è¯•æ ·æœ¬: {len(test_samples)} ä¸ª")
        return test_samples
    
    def create_test_prompts(self, dataset_name: str, 
                          test_samples: List[TestSample]) -> List[str]:
        """
        ä¸ºæµ‹è¯•æ ·æœ¬åˆ›å»ºæç¤ºè¯
        
        ä½¿ç”¨æ–°çš„æç¤ºè¯ç®¡ç†ç³»ç»Ÿï¼Œä»é…ç½®æ–‡ä»¶ä¸­åŠ è½½æç¤ºè¯æ¨¡æ¿
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
            
        Returns:
            List[str]: æ ¼å¼åŒ–çš„æç¤ºè¯åˆ—è¡¨
        """
        prompts = []
        
        try:
            # ä½¿ç”¨MDæç¤ºè¯ç®¡ç†å™¨
            md_manager = get_md_prompt_manager()
            
            for sample in test_samples:
                # å‡†å¤‡æ ·æœ¬æ•°æ®
                sample_data = {
                    'content': sample.content,
                    'category': sample.category,
                    'expected_score': sample.expected_score
                }
                
                # ä½¿ç”¨MDæç¤ºè¯ç®¡ç†å™¨æ¸²æŸ“æç¤ºè¯
                prompt = md_manager.render_prompt(dataset_name, sample_data)
                prompts.append(prompt)
                
            self.logger.info(f"ä½¿ç”¨MDæç¤ºè¯ç®¡ç†å™¨ä¸º {dataset_name} ç”Ÿæˆäº† {len(prompts)} ä¸ªæç¤ºè¯")
            
        except Exception as e:
            self.logger.warning(f"MDæç¤ºè¯ç®¡ç†å™¨å¤±è´¥ï¼Œä½¿ç”¨ç¡¬ç¼–ç å¤‡ç”¨æ–¹æ³•: {str(e)}")
            # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨åŸå§‹ç¡¬ç¼–ç é€»è¾‘
            prompts = self._create_fallback_prompts(dataset_name, test_samples)
        
        return prompts
    
    def _create_fallback_prompts(self, dataset_name: str, 
                               test_samples: List[TestSample]) -> List[str]:
        """
        å¤‡ç”¨æç¤ºè¯åˆ›å»ºæ–¹æ³•ï¼ˆä¿æŒä¸åŸå§‹é€»è¾‘å…¼å®¹ï¼‰
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
            
        Returns:
            List[str]: æ ¼å¼åŒ–çš„æç¤ºè¯åˆ—è¡¨
        """
        prompts = []
        
        for sample in test_samples:
            if dataset_name == 'call_semantic_complaints':
                prompt = f"""è¯·åˆ†æä»¥ä¸‹é€šè¯è¯­ä¹‰å†…å®¹ï¼Œå¹¶è¿›è¡ŒæŠ•è¯‰åˆ†ç±»è¯„ä¼°ï¼š

é€šè¯å†…å®¹ï¼š{sample.content}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼š
1. åˆ¤æ–­æŠ•è¯‰ç±»åˆ«ï¼šæœåŠ¡è´¨é‡æŠ•è¯‰ã€äº§å“åŠŸèƒ½æŠ•è¯‰ã€æ”¶è´¹äº‰è®®æŠ•è¯‰ã€æŠ€æœ¯æ•…éšœæŠ•è¯‰ã€æ€åº¦é—®é¢˜æŠ•è¯‰ã€æ— æŠ•è¯‰å†…å®¹
2. è¯„ä¼°æŠ•è¯‰ä¸¥é‡åº¦ï¼ˆ1-5åˆ†ï¼‰ï¼š1åˆ†=è½»å¾®ä¸æ»¡ï¼Œ2åˆ†=ä¸€èˆ¬æŠ•è¯‰ï¼Œ3åˆ†=ä¸¥é‡æŠ•è¯‰ï¼Œ4åˆ†=éå¸¸ä¸¥é‡ï¼Œ5åˆ†=æåº¦ä¸¥é‡
3. æä¾›ç®€è¦åˆ†æè¯´æ˜

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
  "category": "æŠ•è¯‰ç±»åˆ«",
  "severity_score": è¯„åˆ†æ•°å­—,
  "analysis": "åˆ†æè¯´æ˜"
}}"""
            
            elif dataset_name == 'manga_comment_ad_detection':
                prompt = f"""è¯·åˆ†æä»¥ä¸‹æ¼«ç”»è¯„è®ºï¼Œåˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Šå†…å®¹ï¼š

è¯„è®ºå†…å®¹ï¼š{sample.content}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚è¿›è¡Œåˆ†æï¼š
1. åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Šï¼ˆ0-5åˆ†ï¼‰ï¼š0åˆ†=æ­£å¸¸è¯„è®ºï¼Œ1åˆ†=ç–‘ä¼¼å¹¿å‘Šï¼Œ2åˆ†=å¯èƒ½æ˜¯å¹¿å‘Šï¼Œ3åˆ†=å¾ˆå¯èƒ½æ˜¯å¹¿å‘Šï¼Œ4åˆ†=ç¡®å®šæ˜¯å¹¿å‘Šï¼Œ5åˆ†=æ˜æ˜¾åƒåœ¾å¹¿å‘Š
2. å¹¿å‘Šç±»å‹åˆ†ç±»ï¼šæ­£å¸¸è¯„è®ºã€APPæ¨å¹¿å¹¿å‘Šã€QQ/å¾®ä¿¡è”ç³»å¹¿å‘Šã€ç½‘ç«™æ¨å¹¿å¹¿å‘Šã€è¯ˆéª—ç±»å¹¿å‘Šã€è‰²æƒ…å¼•æµå¹¿å‘Šã€æ¸¸æˆæ¨å¹¿å¹¿å‘Šã€å…¶ä»–å•†ä¸šå¹¿å‘Š
3. æä¾›åˆ¤æ–­ä¾æ®è¯´æ˜

è¯·ä»¥JSONæ ¼å¼å›å¤ï¼š
{{
  "ad_score": è¯„åˆ†æ•°å­—,
  "category": "å¹¿å‘Šç±»å‹",
  "analysis": "åˆ¤æ–­ä¾æ®è¯´æ˜"
}}"""
            
            else:
                # é€šç”¨æ ¼å¼
                prompt = f"""è¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š

å†…å®¹ï¼š{sample.content}

é¢„æœŸç±»åˆ«ï¼š{sample.category}
é¢„æœŸè¯„åˆ†ï¼š{sample.expected_score}

è¯·æä¾›ä½ çš„åˆ†æç»“æœã€‚"""
            
            prompts.append(prompt)
            
        self.logger.info(f"ä½¿ç”¨å¤‡ç”¨æ–¹æ³•ä¸º {dataset_name} ç”Ÿæˆäº† {len(prompts)} ä¸ªæç¤ºè¯")
        return prompts
    
    def evaluate_model_responses(self, dataset_name: str,
                                test_samples: List[TestSample],
                                model_responses: List[Dict[str, Any]]) -> List[TestResult]:
        """
        è¯„ä¼°æ¨¡å‹å“åº”ç»“æœ
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°
            test_samples: æµ‹è¯•æ ·æœ¬åˆ—è¡¨
            model_responses: æ¨¡å‹å“åº”åˆ—è¡¨
            
        Returns:
            List[TestResult]: è¯„ä¼°ç»“æœåˆ—è¡¨
        """
        if len(test_samples) != len(model_responses):
            raise ValueError("æµ‹è¯•æ ·æœ¬æ•°é‡ä¸æ¨¡å‹å“åº”æ•°é‡ä¸åŒ¹é…")
        
        results = []
        
        for sample, response in zip(test_samples, model_responses):
            try:
                # è§£ææ¨¡å‹å“åº”
                model_response_text = response.get('response', '')
                response_time = response.get('latency_ms', 0)
                
                # å°è¯•è§£æJSONå“åº”
                model_result = self._parse_model_response(model_response_text, dataset_name)
                
                # ä½¿ç”¨MDæç¤ºè¯ç®¡ç†å™¨è¿›è¡Œå“åº”è´¨é‡æ£€æŸ¥
                try:
                    md_manager = get_md_prompt_manager()
                    validation_errors = md_manager.validate_response(dataset_name, model_response_text)
                    if validation_errors:
                        self.logger.warning(f"å“åº”è´¨é‡æ£€æŸ¥å‘ç°é—®é¢˜ - æ ·æœ¬ {sample.id}: {'; '.join(validation_errors)}")
                except Exception as e:
                    self.logger.debug(f"å“åº”è´¨é‡æ£€æŸ¥å¤±è´¥ - æ ·æœ¬ {sample.id}: {str(e)}")
                
                if model_result:
                    # è®¡ç®—å‡†ç¡®æ€§
                    score_accuracy = self._calculate_score_accuracy(
                        model_result.get('score'), sample.expected_score
                    )
                    
                    category_match = self._check_category_match(
                        model_result.get('category'), sample.category, dataset_name
                    )
                    
                    test_result = TestResult(
                        sample_id=sample.id,
                        comment=sample.content,  # æ·»åŠ åŸå§‹è¯„è®ºå†…å®¹
                        model_response=model_response_text,
                        model_score=model_result.get('score'),
                        model_category=model_result.get('category'),
                        expected_score=sample.expected_score,
                        expected_category=sample.category,
                        score_accuracy=score_accuracy,
                        category_match=category_match,
                        response_time_ms=response_time
                    )
                else:
                    # è§£æå¤±è´¥
                    test_result = TestResult(
                        sample_id=sample.id,
                        comment=sample.content,  # æ·»åŠ åŸå§‹è¯„è®ºå†…å®¹
                        model_response=model_response_text,
                        model_score=None,
                        model_category=None,
                        expected_score=sample.expected_score,
                        expected_category=sample.category,
                        score_accuracy=0.0,
                        category_match=False,
                        response_time_ms=response_time,
                        error="æ— æ³•è§£ææ¨¡å‹å“åº”"
                    )
                
                results.append(test_result)
                
            except Exception as e:
                error_result = TestResult(
                    sample_id=sample.id,
                    comment=sample.content,  # æ·»åŠ åŸå§‹è¯„è®ºå†…å®¹
                    model_response=str(response),
                    model_score=None,
                    model_category=None,
                    expected_score=sample.expected_score,
                    expected_category=sample.category,
                    score_accuracy=0.0,
                    category_match=False,
                    response_time_ms=0,
                    error=str(e)
                )
                results.append(error_result)
        
        return results
    
    def _parse_model_response(self, response_text: str, dataset_name: str) -> Optional[Dict[str, Any]]:
        """è§£ææ¨¡å‹å“åº”"""
        try:
            # å°è¯•è§£æJSON
            if '{' in response_text and '}' in response_text:
                json_start = response_text.find('{')
                json_end = response_text.rfind('}') + 1
                json_text = response_text[json_start:json_end]
                
                parsed = json.loads(json_text)
                
                if dataset_name == 'call_semantic_complaints':
                    return {
                        'category': parsed.get('category'),
                        'score': parsed.get('severity_score'),
                        'analysis': parsed.get('analysis')
                    }
                elif dataset_name == 'manga_comment_ad_detection':
                    return {
                        'category': parsed.get('category'),
                        'score': parsed.get('ad_score'),
                        'analysis': parsed.get('analysis')
                    }
                else:
                    return parsed
                    
        except Exception as e:
            self.logger.warning(f"JSONè§£æå¤±è´¥: {e}")
        
        # å°è¯•æ–‡æœ¬è§£æ
        try:
            result = {}
            lines = response_text.split('\n')
            
            for line in lines:
                if 'åˆ†' in line or 'è¯„åˆ†' in line:
                    # æå–æ•°å­—
                    import re
                    numbers = re.findall(r'\d+', line)
                    if numbers:
                        result['score'] = int(numbers[0])
                
                if 'ç±»åˆ«' in line or 'æŠ•è¯‰' in line:
                    if 'æœåŠ¡è´¨é‡' in line:
                        result['category'] = 'æœåŠ¡è´¨é‡æŠ•è¯‰'
                    elif 'äº§å“åŠŸèƒ½' in line:
                        result['category'] = 'äº§å“åŠŸèƒ½æŠ•è¯‰'
                    elif 'æ”¶è´¹äº‰è®®' in line:
                        result['category'] = 'æ”¶è´¹äº‰è®®æŠ•è¯‰'
                    elif 'æŠ€æœ¯æ•…éšœ' in line:
                        result['category'] = 'æŠ€æœ¯æ•…éšœæŠ•è¯‰'
                    elif 'æ€åº¦é—®é¢˜' in line:
                        result['category'] = 'æ€åº¦é—®é¢˜æŠ•è¯‰'
                    elif 'æ— æŠ•è¯‰' in line:
                        result['category'] = 'æ— æŠ•è¯‰å†…å®¹'
                
                # å¹¿å‘Šæ£€æµ‹çš„ç±»åˆ«åŒ¹é…
                if 'æ­£å¸¸è¯„è®º' in line:
                    result['category'] = 'æ­£å¸¸è¯„è®º'
                elif 'APPæ¨å¹¿' in line or 'appæ¨å¹¿' in line:
                    result['category'] = 'APPæ¨å¹¿å¹¿å‘Š'
                elif 'QQ' in line or 'å¾®ä¿¡' in line:
                    result['category'] = 'QQ/å¾®ä¿¡è”ç³»å¹¿å‘Š'
                elif 'ç½‘ç«™æ¨å¹¿' in line or 'ç½‘ç«™' in line:
                    result['category'] = 'ç½‘ç«™æ¨å¹¿å¹¿å‘Š'
                elif 'è¯ˆéª—' in line or 'ä¸­å¥–' in line:
                    result['category'] = 'è¯ˆéª—ç±»å¹¿å‘Š'
                elif 'è‰²æƒ…' in line or 'ç›´æ’­' in line:
                    result['category'] = 'è‰²æƒ…å¼•æµå¹¿å‘Š'
                elif 'æ¸¸æˆ' in line:
                    result['category'] = 'æ¸¸æˆæ¨å¹¿å¹¿å‘Š'
                elif 'å•†ä¸šå¹¿å‘Š' in line or 'å¹¿å‘Š' in line:
                    result['category'] = 'å…¶ä»–å•†ä¸šå¹¿å‘Š'
                
                # åŸæœ‰çš„æ¨å¹¿ç­‰çº§åŒ¹é…ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
                if 'Açº§' in line or 'Bçº§' in line or 'Cçº§' in line or 'Dçº§' in line or 'Eçº§' in line:
                    import re
                    match = re.search(r'([ABCDE])çº§', line)
                    if match:
                        result['category'] = f"{match.group(1)}çº§"
            
            return result if result else None
            
        except Exception:
            return None
    
    def _calculate_score_accuracy(self, model_score: Optional[float], 
                                expected_score: float) -> float:
        """è®¡ç®—è¯„åˆ†å‡†ç¡®æ€§"""
        if model_score is None:
            return 0.0
        
        # è®¡ç®—ç›¸å¯¹è¯¯å·®çš„å‡†ç¡®æ€§
        max_diff = max(5, expected_score)  # å‡è®¾æœ€å¤§è¯„åˆ†æ˜¯5
        diff = abs(model_score - expected_score)
        accuracy = max(0, 1 - diff / max_diff)
        
        return accuracy
    
    def _check_category_match(self, model_category: Optional[str], 
                            expected_category: str, dataset_name: str) -> bool:
        """æ£€æŸ¥ç±»åˆ«åŒ¹é…"""
        if not model_category:
            return False
        
        # ç²¾ç¡®åŒ¹é…
        if model_category == expected_category:
            return True
        
        # æ¨¡ç³ŠåŒ¹é…
        if dataset_name == 'call_semantic_complaints':
            # æŠ•è¯‰åˆ†ç±»çš„æ¨¡ç³ŠåŒ¹é…
            category_keywords = {
                'æœåŠ¡è´¨é‡æŠ•è¯‰': ['æœåŠ¡', 'è´¨é‡'],
                'äº§å“åŠŸèƒ½æŠ•è¯‰': ['äº§å“', 'åŠŸèƒ½'],
                'æ”¶è´¹äº‰è®®æŠ•è¯‰': ['æ”¶è´¹', 'äº‰è®®', 'è´¹ç”¨'],
                'æŠ€æœ¯æ•…éšœæŠ•è¯‰': ['æŠ€æœ¯', 'æ•…éšœ'],
                'æ€åº¦é—®é¢˜æŠ•è¯‰': ['æ€åº¦', 'é—®é¢˜'],
                'æ— æŠ•è¯‰å†…å®¹': ['æ— æŠ•è¯‰', 'æ— ']
            }
            
            expected_keywords = category_keywords.get(expected_category, [])
            for keyword in expected_keywords:
                if keyword in model_category:
                    return True
        
        elif dataset_name == 'user_comment_ad_evaluation':
            # å¹¿å‘Šæ¨å¹¿ç­‰çº§çš„æ¨¡ç³ŠåŒ¹é…
            if expected_category in model_category or model_category in expected_category:
                return True
        
        return False
    
    def generate_evaluation_report(self, dataset_name: str, model_name: str,
                                 test_results: List[TestResult]) -> EvaluationReport:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        
        Args:
            dataset_name: æµ‹è¯•é›†åç§°
            model_name: æ¨¡å‹åç§°
            test_results: æµ‹è¯•ç»“æœåˆ—è¡¨
            
        Returns:
            EvaluationReport: è¯„ä¼°æŠ¥å‘Š
        """
        total_samples = len(test_results)
        successful_tests = len([r for r in test_results if r.error is None])
        failed_tests = total_samples - successful_tests
        
        # è®¡ç®—å‡†ç¡®æ€§æŒ‡æ ‡
        valid_results = [r for r in test_results if r.error is None and r.model_score is not None]
        
        if valid_results:
            avg_score_accuracy = statistics.mean([r.score_accuracy for r in valid_results])
            category_accuracy = sum([1 for r in valid_results if r.category_match]) / len(valid_results)
            avg_response_time = statistics.mean([r.response_time_ms for r in test_results])
        else:
            avg_score_accuracy = 0.0
            category_accuracy = 0.0
            avg_response_time = 0.0
        
        # åˆ†æ•°åˆ†å¸ƒç»Ÿè®¡
        score_distribution = {}
        for result in valid_results:
            score_range = f"{int(result.model_score)}-{int(result.model_score)+1}"
            score_distribution[score_range] = score_distribution.get(score_range, 0) + 1
        
        # ç±»åˆ«åˆ†å¸ƒç»Ÿè®¡
        category_distribution = {}
        for result in test_results:
            category = result.expected_category
            category_distribution[category] = category_distribution.get(category, 0) + 1
        
        return EvaluationReport(
            dataset_name=dataset_name,
            model_name=model_name,
            test_time=datetime.now().isoformat(),
            total_samples=total_samples,
            successful_tests=successful_tests,
            failed_tests=failed_tests,
            avg_score_accuracy=avg_score_accuracy,
            category_accuracy=category_accuracy,
            avg_response_time_ms=avg_response_time,
            score_distribution=score_distribution,
            category_distribution=category_distribution,
            detailed_results=test_results
        )
    
    def save_evaluation_report(self, report: EvaluationReport, 
                             output_dir: str = "./test_results") -> str:
        """
        ä¿å­˜è¯„ä¼°æŠ¥å‘Š
        
        Args:
            report: è¯„ä¼°æŠ¥å‘Š
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            str: ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
        """
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{report.dataset_name}_{report.model_name}_{timestamp}_evaluation.json"
        
        report_file = output_path / filename
        
        try:
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(asdict(report), f, indent=2, ensure_ascii=False, default=str)
            
            self.logger.info(f"è¯„ä¼°æŠ¥å‘Šå·²ä¿å­˜: {report_file}")
            return str(report_file)
            
        except Exception as e:
            self.logger.error(f"ä¿å­˜è¯„ä¼°æŠ¥å‘Šå¤±è´¥: {e}")
            raise
    
    def generate_html_report(self, report: EvaluationReport, 
                           output_dir: str = "./test_results") -> str:
        """
        ç”ŸæˆHTMLæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š
        
        Args:
            report: è¯„ä¼°æŠ¥å‘Š
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            str: HTMLæŠ¥å‘Šæ–‡ä»¶è·¯å¾„
        """
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>{dataset_name} - {model_name} æµ‹è¯•è¯„ä¼°æŠ¥å‘Š</title>
            <style>
                body {{ 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                    margin: 20px; 
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }}
                h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
                h2 {{ color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                .metric-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }}
                .metric {{ 
                    padding: 15px; 
                    background: #f8f9fa; 
                    border-radius: 8px;
                    border-left: 4px solid #3498db;
                }}
                .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 5px; }}
                .metric-value {{ font-size: 1.2em; color: #27ae60; }}
                .good {{ color: #27ae60; }}
                .warning {{ color: #f39c12; }}
                .error {{ color: #e74c3c; }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                th, td {{
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{ background-color: #3498db; color: white; }}
                .footer {{
                    margin-top: 30px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    color: #7f8c8d;
                    text-align: center;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸ“Š {dataset_name} - {model_name} æµ‹è¯•è¯„ä¼°æŠ¥å‘Š</h1>
                
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•é›†</div>
                        <div class="metric-value">{dataset_name}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•æ¨¡å‹</div>
                        <div class="metric-value">{model_name}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•æ—¶é—´</div>
                        <div class="metric-value">{test_time}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æ€»æ ·æœ¬æ•°</div>
                        <div class="metric-value">{total_samples}</div>
                    </div>
                </div>

                <h2>ğŸ“ˆ æ ¸å¿ƒè¯„ä¼°æŒ‡æ ‡</h2>
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">æˆåŠŸæµ‹è¯•æ•°</div>
                        <div class="metric-value good">{successful_tests}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">å¤±è´¥æµ‹è¯•æ•°</div>
                        <div class="metric-value {failed_class}">{failed_tests}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">è¯„åˆ†å‡†ç¡®æ€§</div>
                        <div class="metric-value {score_class}">{avg_score_accuracy:.2%}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">ç±»åˆ«å‡†ç¡®æ€§</div>
                        <div class="metric-value {category_class}">{category_accuracy:.2%}</div>
                    </div>
                </div>

                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">å¹³å‡å“åº”æ—¶é—´</div>
                        <div class="metric-value">{avg_response_time_ms:.2f}ms</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æˆåŠŸç‡</div>
                        <div class="metric-value {success_class}">{success_rate:.2%}</div>
                    </div>
                </div>

                <h2>ğŸ“Š åˆ†å¸ƒç»Ÿè®¡</h2>
                <table>
                    <tr>
                        <th>ç±»åˆ«</th>
                        <th>æ ·æœ¬æ•°é‡</th>
                        <th>å æ¯”</th>
                    </tr>
                    {category_rows}
                </table>

                <div class="footer">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}</p>
                    <p>ğŸ¤– ç”± Qwen-3 æµ‹è¯•é›†ç®¡ç†å™¨ç”Ÿæˆ</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # è®¡ç®—æ ·å¼ç±»
        failed_class = "error" if report.failed_tests > 0 else "good"
        score_class = "good" if report.avg_score_accuracy > 0.8 else "warning" if report.avg_score_accuracy > 0.6 else "error"
        category_class = "good" if report.category_accuracy > 0.8 else "warning" if report.category_accuracy > 0.6 else "error"
        success_rate = report.successful_tests / report.total_samples if report.total_samples > 0 else 0
        success_class = "good" if success_rate > 0.9 else "warning" if success_rate > 0.7 else "error"
        
        # ç”Ÿæˆç±»åˆ«åˆ†å¸ƒè¡¨æ ¼
        category_rows = ""
        for category, count in report.category_distribution.items():
            percentage = count / report.total_samples * 100 if report.total_samples > 0 else 0
            category_rows += f"""
            <tr>
                <td>{category}</td>
                <td>{count}</td>
                <td>{percentage:.1f}%</td>
            </tr>
            """
        
        html_content = html_template.format(
            dataset_name=report.dataset_name,
            model_name=report.model_name,
            test_time=report.test_time,
            total_samples=report.total_samples,
            successful_tests=report.successful_tests,
            failed_tests=report.failed_tests,
            failed_class=failed_class,
            avg_score_accuracy=report.avg_score_accuracy,
            score_class=score_class,
            category_accuracy=report.category_accuracy,
            category_class=category_class,
            avg_response_time_ms=report.avg_response_time_ms,
            success_rate=success_rate,
            success_class=success_class,
            category_rows=category_rows,
            report_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        filename = f"{report.dataset_name}_{report.model_name}_{timestamp}_report.html"
        
        report_file = output_path / filename
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"HTMLè¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return str(report_file)
    
    def list_available_datasets(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æµ‹è¯•é›†"""
        datasets = []
        for file_path in self.datasets_dir.glob("*.json"):
            datasets.append(file_path.stem)
        return sorted(datasets)
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict[str, Any]]:
        """è·å–æµ‹è¯•é›†ä¿¡æ¯"""
        dataset = self.load_dataset(dataset_name)
        if dataset:
            return dataset.get('dataset_info')
        return None


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºæµ‹è¯•é›†ç®¡ç†å™¨
    manager = TestDatasetManager()
    
    # åˆ—å‡ºå¯ç”¨æµ‹è¯•é›†
    print("ğŸ“‹ å¯ç”¨æµ‹è¯•é›†:")
    for dataset in manager.list_available_datasets():
        info = manager.get_dataset_info(dataset)
        if info:
            print(f"  - {dataset}: {info.get('name', 'N/A')} ({info.get('total_samples', 0)} æ ·æœ¬)")
    
    # æ¼”ç¤ºåŠ è½½å’Œä½¿ç”¨æµ‹è¯•é›†
    if len(sys.argv) > 1:
        dataset_name = sys.argv[1]
        print(f"\nğŸ” åŠ è½½æµ‹è¯•é›†: {dataset_name}")
        
        # è·å–æµ‹è¯•æ ·æœ¬
        samples = manager.get_test_samples(dataset_name, sample_count=5)
        if samples:
            print(f"âœ… è·å–åˆ° {len(samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
            
            # åˆ›å»ºæµ‹è¯•æç¤ºè¯
            prompts = manager.create_test_prompts(dataset_name, samples)
            print(f"âœ… ç”Ÿæˆ {len(prompts)} ä¸ªæµ‹è¯•æç¤ºè¯")
            
            # æ˜¾ç¤ºç¬¬ä¸€ä¸ªæ ·æœ¬
            print(f"\nğŸ“ ç¬¬ä¸€ä¸ªæµ‹è¯•æ ·æœ¬:")
            print(f"ID: {samples[0].id}")
            print(f"ç±»åˆ«: {samples[0].category}")
            print(f"é¢„æœŸè¯„åˆ†: {samples[0].expected_score}")
            print(f"å†…å®¹: {samples[0].content[:100]}...")
            print(f"\næç¤ºè¯: {prompts[0][:200]}...")
        else:
            print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ ·æœ¬")
    else:
        print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•: python test_dataset_manager.py <dataset_name>")
        print("ä¾‹å¦‚: python test_dataset_manager.py call_semantic_complaints")