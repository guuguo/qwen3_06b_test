#!/usr/bin/env python3
"""
Qwen-3 Ollama é›†æˆæ¨¡å—

æä¾›ä¸ Ollama æœåŠ¡çš„å®Œæ•´é›†æˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æœåŠ¡çŠ¶æ€æ£€æŸ¥å’Œå¥åº·ç›‘æ§
- æ¨¡å‹ç®¡ç†å’Œåˆ—è¡¨è·å–
- å¸¦æŒ‡æ ‡æ”¶é›†çš„æ¨ç†æ¥å£
- é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
- è¿æ¥æ± ç®¡ç†å’Œæ€§èƒ½ä¼˜åŒ–

ä½œè€…: Qwen-3 éƒ¨ç½²å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
"""

import json
import time
import logging
import threading
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from urllib.parse import urljoin
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry


@dataclass
class InferenceMetrics:
    """æ¨ç†æŒ‡æ ‡æ•°æ®ç±»"""
    model: str
    prompt_length: int
    response_length: int
    latency_ms: float
    status: str
    timestamp: str
    error: Optional[str] = None
    tokens_per_second: Optional[float] = None
    memory_usage_mb: Optional[float] = None


@dataclass
class HealthStatus:
    """å¥åº·çŠ¶æ€æ•°æ®ç±»"""
    status: str  # healthy, unhealthy, degraded
    latency_ms: float
    models_count: int
    models: List[str]
    timestamp: str
    error: Optional[str] = None
    uptime_seconds: Optional[float] = None


class OllamaConnectionError(Exception):
    """Ollama è¿æ¥é”™è¯¯"""
    pass


class OllamaModelError(Exception):
    """Ollama æ¨¡å‹é”™è¯¯"""
    pass


class OllamaIntegration:
    """
    Ollama é›†æˆæ ¸å¿ƒç±»
    
    æä¾›ä¸ Ollama æœåŠ¡çš„å®Œæ•´é›†æˆåŠŸèƒ½ï¼ŒåŒ…æ‹¬è¿æ¥ç®¡ç†ã€
    æ¨¡å‹æ“ä½œã€æ¨ç†æœåŠ¡å’Œæ€§èƒ½ç›‘æ§ã€‚
    """
    
    def __init__(self, 
                 base_url: str = "http://localhost:11434",
                 timeout: int = 30,
                 max_retries: int = 3,
                 retry_backoff_factor: float = 0.3,
                 pool_connections: int = 10,
                 pool_maxsize: int = 10):
        """
        åˆå§‹åŒ– Ollama é›†æˆ
        
        Args:
            base_url: Ollama æœåŠ¡åŸºç¡€ URL
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            retry_backoff_factor: é‡è¯•é€€é¿å› å­
            pool_connections: è¿æ¥æ± è¿æ¥æ•°
            pool_maxsize: è¿æ¥æ± æœ€å¤§å¤§å°
        """
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.max_retries = max_retries
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # åˆ›å»ºä¼šè¯å’Œè¿æ¥æ± 
        self.session = self._create_session(
            max_retries, retry_backoff_factor, 
            pool_connections, pool_maxsize
        )
        
        # æ€§èƒ½ç»Ÿè®¡
        self._stats = {
            'total_requests': 0,
            'successful_requests': 0,
            'failed_requests': 0,
            'total_latency': 0.0,
            'start_time': time.time()
        }
        self._stats_lock = threading.Lock()
        
        # ç¼“å­˜
        self._models_cache = None
        self._models_cache_time = 0
        self._cache_ttl = 300  # 5åˆ†é’Ÿç¼“å­˜
        
        self.logger.info(f"Ollama é›†æˆåˆå§‹åŒ–å®Œæˆ: {self.base_url}")
    
    def _create_session(self, max_retries: int, backoff_factor: float,
                       pool_connections: int, pool_maxsize: int) -> requests.Session:
        """åˆ›å»ºé…ç½®å¥½çš„ requests ä¼šè¯"""
        session = requests.Session()
        
        # é…ç½®é‡è¯•ç­–ç•¥
        retry_strategy = Retry(
            total=max_retries,
            backoff_factor=backoff_factor,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["HEAD", "GET", "POST"]
        )
        
        # é…ç½®é€‚é…å™¨
        adapter = HTTPAdapter(
            max_retries=retry_strategy,
            pool_connections=pool_connections,
            pool_maxsize=pool_maxsize
        )
        
        session.mount("http://", adapter)
        session.mount("https://", adapter)
        
        # è®¾ç½®é»˜è®¤å¤´éƒ¨
        session.headers.update({
            'Content-Type': 'application/json',
            'User-Agent': 'Qwen3-Integration/1.0.0'
        })
        
        return session
    
    def _update_stats(self, success: bool, latency: float):
        """æ›´æ–°æ€§èƒ½ç»Ÿè®¡"""
        with self._stats_lock:
            self._stats['total_requests'] += 1
            self._stats['total_latency'] += latency
            
            if success:
                self._stats['successful_requests'] += 1
            else:
                self._stats['failed_requests'] += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½ç»Ÿè®¡ä¿¡æ¯"""
        with self._stats_lock:
            stats = self._stats.copy()
            
        uptime = time.time() - stats['start_time']
        avg_latency = (stats['total_latency'] / stats['total_requests'] 
                      if stats['total_requests'] > 0 else 0)
        success_rate = (stats['successful_requests'] / stats['total_requests'] 
                       if stats['total_requests'] > 0 else 0)
        
        return {
            **stats,
            'uptime_seconds': uptime,
            'average_latency_ms': avg_latency * 1000,
            'success_rate': success_rate,
            'requests_per_second': stats['total_requests'] / uptime if uptime > 0 else 0
        }
    
    def check_ollama_status(self) -> bool:
        """
        æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€
        
        Returns:
            bool: æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œ
        """
        try:
            start_time = time.time()
            response = self.session.get(
                urljoin(self.base_url, '/api/tags'),
                timeout=5  # ä½¿ç”¨è¾ƒçŸ­çš„è¶…æ—¶æ—¶é—´è¿›è¡Œå¿«é€Ÿæ£€æŸ¥
            )
            latency = time.time() - start_time
            
            success = response.status_code == 200
            self._update_stats(success, latency)
            
            if success:
                self.logger.debug(f"Ollama æœåŠ¡çŠ¶æ€æ£€æŸ¥æˆåŠŸï¼Œå»¶è¿Ÿ: {latency*1000:.2f}ms")
                return True
            else:
                self.logger.warning(f"Ollama æœåŠ¡çŠ¶æ€æ£€æŸ¥å¤±è´¥: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            latency = time.time() - start_time if 'start_time' in locals() else 0
            self._update_stats(False, latency)
            self.logger.error(f"Ollama æœåŠ¡çŠ¶æ€æ£€æŸ¥å¼‚å¸¸: {e}")
            return False
    
    def list_models(self, use_cache: bool = False) -> List[str]:
        """
        è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨
        
        Args:
            use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜ï¼ˆé»˜è®¤Falseï¼Œå› ä¸ºæ¨¡å‹åˆ—è¡¨æŸ¥è¯¢å¾ˆå¿«ä¸”éœ€è¦å®æ—¶æ€§ï¼‰
            
        Returns:
            List[str]: æ¨¡å‹åç§°åˆ—è¡¨
            
        Raises:
            OllamaConnectionError: è¿æ¥å¤±è´¥
        """
        # æ£€æŸ¥ç¼“å­˜ï¼ˆä»…åœ¨æ˜ç¡®è¦æ±‚æ—¶ä½¿ç”¨ï¼‰
        if use_cache and self._models_cache is not None:
            cache_age = time.time() - self._models_cache_time
            if cache_age < self._cache_ttl:
                self.logger.debug(f"ä½¿ç”¨ç¼“å­˜çš„æ¨¡å‹åˆ—è¡¨ï¼Œç¼“å­˜å¹´é¾„: {cache_age:.1f}s")
                return self._models_cache
        
        try:
            start_time = time.time()
            response = self.session.get(
                urljoin(self.base_url, '/api/tags'),
                timeout=self.timeout
            )
            latency = time.time() - start_time
            
            if response.status_code == 200:
                data = response.json()
                models = [model['name'] for model in data.get('models', [])]
                
                # æ›´æ–°ç¼“å­˜ï¼ˆå¤‡ç”¨ï¼‰
                self._models_cache = models
                self._models_cache_time = time.time()
                
                self._update_stats(True, latency)
                self.logger.debug(f"è·å–æ¨¡å‹åˆ—è¡¨æˆåŠŸ: {len(models)} ä¸ªæ¨¡å‹ (è€—æ—¶: {latency*1000:.1f}ms)")
                return models
            else:
                self._update_stats(False, latency)
                error_msg = f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: HTTP {response.status_code}"
                self.logger.error(error_msg)
                raise OllamaConnectionError(error_msg)
                
        except requests.exceptions.RequestException as e:
            latency = time.time() - start_time if 'start_time' in locals() else 0
            self._update_stats(False, latency)
            error_msg = f"è·å–æ¨¡å‹åˆ—è¡¨ç½‘ç»œé”™è¯¯: {e}"
            self.logger.error(error_msg)
            raise OllamaConnectionError(error_msg)
    
    def model_exists(self, model_name: str) -> bool:
        """
        æ£€æŸ¥æ¨¡å‹æ˜¯å¦å­˜åœ¨
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            bool: æ¨¡å‹æ˜¯å¦å­˜åœ¨
        """
        try:
            models = self.list_models()
            return model_name in models
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥æ¨¡å‹å­˜åœ¨æ€§å¤±è´¥: {e}")
            return False
    
    def inference_with_metrics(self, 
                             model: str, 
                             prompt: str,
                             stream: bool = False,
                             enable_thinking: bool = True,
                             **options) -> InferenceMetrics:
        """
        å¸¦æŒ‡æ ‡æ”¶é›†çš„æ¨ç†æ¥å£
        
        Args:
            model: æ¨¡å‹åç§°
            prompt: è¾“å…¥æç¤º
            stream: æ˜¯å¦æµå¼è¾“å‡º
            enable_thinking: æ˜¯å¦å¯ç”¨æ¨¡å‹æ€è€ƒè¿‡ç¨‹
            **options: å…¶ä»–æ¨ç†é€‰é¡¹
            
        Returns:
            InferenceMetrics: æ¨ç†ç»“æœå’ŒæŒ‡æ ‡
            
        Raises:
            OllamaModelError: æ¨¡å‹ç›¸å…³é”™è¯¯
            OllamaConnectionError: è¿æ¥é”™è¯¯
        """
        start_time = time.time()
        
        # éªŒè¯æ¨¡å‹å­˜åœ¨æ€§
        if not self.model_exists(model):
            error_msg = f"æ¨¡å‹ä¸å­˜åœ¨: {model}"
            self.logger.error(error_msg)
            return InferenceMetrics(
                model=model,
                prompt_length=len(prompt),
                response_length=0,
                latency_ms=0,
                status="error",
                timestamp=datetime.now().isoformat(),
                error=error_msg
            )
        
        request_data = {
            "model": model,
            "prompt": prompt,
            "stream": stream,
            "think": enable_thinking,  # ç›´æ¥æ§åˆ¶æ€è€ƒå¼€å…³
            **options
        }
        
        # è®°å½•æ€è€ƒå¼€å…³çŠ¶æ€
        thinking_status = "å¯ç”¨" if enable_thinking else "ç¦ç”¨"
        self.logger.info(f"æ¨¡å‹æ¨ç†è¯·æ±‚ - æ¨¡å‹: {model}, æ€è€ƒ: {thinking_status} (think: {enable_thinking})")
        
        try:
            response = self.session.post(
                urljoin(self.base_url, '/api/generate'),
                json=request_data,
                timeout=self.timeout
            )
            
            end_time = time.time()
            latency = end_time - start_time
            
            if response.status_code == 200:
                result_data = response.json()
                response_text = result_data.get("response", "")
                
                # è®¡ç®— tokens per second (ä¼°ç®—)
                response_tokens = len(response_text.split())
                tokens_per_second = response_tokens / latency if latency > 0 else 0
                
                metrics = InferenceMetrics(
                    model=model,
                    prompt_length=len(prompt),
                    response_length=len(response_text),
                    latency_ms=latency * 1000,
                    status="success",
                    timestamp=datetime.now().isoformat(),
                    tokens_per_second=tokens_per_second
                )
                
                self._update_stats(True, latency)
                self.logger.info(f"æ¨ç†æˆåŠŸ: {model}, å»¶è¿Ÿ: {latency*1000:.2f}ms, "
                               f"TPS: {tokens_per_second:.2f}")
                
                # æ·»åŠ å“åº”å†…å®¹åˆ°æŒ‡æ ‡ä¸­ï¼ˆç”¨äºè¿”å›ç»™è°ƒç”¨è€…ï¼‰
                metrics_dict = asdict(metrics)
                metrics_dict['response'] = response_text
                return metrics_dict
                
            else:
                error_msg = f"æ¨ç†è¯·æ±‚å¤±è´¥: HTTP {response.status_code} - {response.text}"
                self.logger.error(error_msg)
                self._update_stats(False, latency)
                
                return InferenceMetrics(
                    model=model,
                    prompt_length=len(prompt),
                    response_length=0,
                    latency_ms=latency * 1000,
                    status="error",
                    timestamp=datetime.now().isoformat(),
                    error=error_msg
                )
                
        except requests.exceptions.Timeout:
            latency = time.time() - start_time
            error_msg = f"æ¨ç†è¯·æ±‚è¶…æ—¶: {self.timeout}s"
            self.logger.error(error_msg)
            self._update_stats(False, latency)
            
            return InferenceMetrics(
                model=model,
                prompt_length=len(prompt),
                response_length=0,
                latency_ms=latency * 1000,
                status="timeout",
                timestamp=datetime.now().isoformat(),
                error=error_msg
            )
            
        except requests.exceptions.RequestException as e:
            latency = time.time() - start_time
            error_msg = f"æ¨ç†è¯·æ±‚ç½‘ç»œé”™è¯¯: {e}"
            self.logger.error(error_msg)
            self._update_stats(False, latency)
            
            return InferenceMetrics(
                model=model,
                prompt_length=len(prompt),
                response_length=0,
                latency_ms=latency * 1000,
                status="error",
                timestamp=datetime.now().isoformat(),
                error=error_msg
            )
    
    def health_check(self) -> HealthStatus:
        """
        ç»¼åˆå¥åº·æ£€æŸ¥
        
        Returns:
            HealthStatus: è¯¦ç»†çš„å¥åº·çŠ¶æ€ä¿¡æ¯
        """
        start_time = time.time()
        
        try:
            # æ£€æŸ¥æœåŠ¡çŠ¶æ€
            response = self.session.get(
                urljoin(self.base_url, '/api/tags'),
                timeout=10
            )
            latency = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                # è·å–æ¨¡å‹åˆ—è¡¨
                try:
                    models = self.list_models(use_cache=False)
                    models_count = len(models)
                    
                    # åˆ¤æ–­å¥åº·çŠ¶æ€
                    if models_count == 0:
                        status = "degraded"
                        error = "æ²¡æœ‰å¯ç”¨çš„æ¨¡å‹"
                    elif latency > 5000:  # 5ç§’ä»¥ä¸Šè®¤ä¸ºæ€§èƒ½é™çº§
                        status = "degraded"
                        error = f"å“åº”å»¶è¿Ÿè¿‡é«˜: {latency:.2f}ms"
                    else:
                        status = "healthy"
                        error = None
                        
                except Exception as e:
                    status = "degraded"
                    models = []
                    models_count = 0
                    error = f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}"
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                stats = self.get_stats()
                
                return HealthStatus(
                    status=status,
                    latency_ms=latency,
                    models_count=models_count,
                    models=models,
                    timestamp=datetime.now().isoformat(),
                    error=error,
                    uptime_seconds=stats['uptime_seconds']
                )
                
            else:
                return HealthStatus(
                    status="unhealthy",
                    latency_ms=latency,
                    models_count=0,
                    models=[],
                    timestamp=datetime.now().isoformat(),
                    error=f"HTTP {response.status_code}: {response.text}"
                )
                
        except Exception as e:
            latency = (time.time() - start_time) * 1000
            return HealthStatus(
                status="unhealthy",
                latency_ms=latency,
                models_count=0,
                models=[],
                timestamp=datetime.now().isoformat(),
                error=str(e)
            )
    
    def pull_model(self, model_name: str, timeout: Optional[int] = None) -> bool:
        """
        æ‹‰å–æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ï¼ŒNone ä½¿ç”¨é»˜è®¤å€¼
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¼€å§‹æ‹‰å–æ¨¡å‹: {model_name}")
            
            response = self.session.post(
                urljoin(self.base_url, '/api/pull'),
                json={"name": model_name},
                timeout=timeout or 600  # é»˜è®¤10åˆ†é’Ÿè¶…æ—¶
            )
            
            if response.status_code == 200:
                self.logger.info(f"æ¨¡å‹æ‹‰å–æˆåŠŸ: {model_name}")
                # æ¸…é™¤æ¨¡å‹ç¼“å­˜
                self._models_cache = None
                return True
            else:
                self.logger.error(f"æ¨¡å‹æ‹‰å–å¤±è´¥: {model_name}, HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹æ‹‰å–å¼‚å¸¸: {model_name}, {e}")
            return False
    
    def delete_model(self, model_name: str) -> bool:
        """
        åˆ é™¤æ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            self.logger.info(f"å¼€å§‹åˆ é™¤æ¨¡å‹: {model_name}")
            
            response = self.session.delete(
                urljoin(self.base_url, '/api/delete'),
                json={"name": model_name},
                timeout=self.timeout
            )
            
            if response.status_code == 200:
                self.logger.info(f"æ¨¡å‹åˆ é™¤æˆåŠŸ: {model_name}")
                # æ¸…é™¤æ¨¡å‹ç¼“å­˜
                self._models_cache = None
                return True
            else:
                self.logger.error(f"æ¨¡å‹åˆ é™¤å¤±è´¥: {model_name}, HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åˆ é™¤å¼‚å¸¸: {model_name}, {e}")
            return False
    
    def close(self):
        """å…³é—­è¿æ¥"""
        if self.session:
            self.session.close()
            self.logger.info("Ollama é›†æˆè¿æ¥å·²å…³é—­")
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.close()


# ä¾¿æ·å‡½æ•°
def create_ollama_client(base_url: str = "http://localhost:11434", **kwargs) -> OllamaIntegration:
    """
    åˆ›å»º Ollama å®¢æˆ·ç«¯çš„ä¾¿æ·å‡½æ•°
    
    Args:
        base_url: Ollama æœåŠ¡åœ°å€
        **kwargs: å…¶ä»–åˆå§‹åŒ–å‚æ•°
        
    Returns:
        OllamaIntegration: é…ç½®å¥½çš„å®¢æˆ·ç«¯å®ä¾‹
    """
    return OllamaIntegration(base_url=base_url, **kwargs)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # åˆ›å»ºå®¢æˆ·ç«¯
    with create_ollama_client() as client:
        print("ğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
        if client.check_ollama_status():
            print("âœ… Ollama æœåŠ¡æ­£å¸¸")
            
            print("\nğŸ“‹ è·å–æ¨¡å‹åˆ—è¡¨...")
            try:
                models = client.list_models()
                print(f"âœ… å‘ç° {len(models)} ä¸ªæ¨¡å‹: {', '.join(models)}")
                
                if models:
                    print(f"\nğŸ§ª æµ‹è¯•æ¨ç† ({models[0]})...")
                    result = client.inference_with_metrics(
                        models[0], 
                        "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
                    )
                    
                    if result['status'] == 'success':
                        print(f"âœ… æ¨ç†æˆåŠŸ!")
                        print(f"   å»¶è¿Ÿ: {result['latency_ms']:.2f}ms")
                        print(f"   TPS: {result.get('tokens_per_second', 0):.2f}")
                        print(f"   å“åº”: {result['response'][:100]}...")
                    else:
                        print(f"âŒ æ¨ç†å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
                
                print(f"\nğŸ“Š å¥åº·æ£€æŸ¥...")
                health = client.health_check()
                print(f"çŠ¶æ€: {health.status}")
                print(f"å»¶è¿Ÿ: {health.latency_ms:.2f}ms")
                print(f"æ¨¡å‹æ•°é‡: {health.models_count}")
                
                print(f"\nğŸ“ˆ æ€§èƒ½ç»Ÿè®¡...")
                stats = client.get_stats()
                print(f"æ€»è¯·æ±‚æ•°: {stats['total_requests']}")
                print(f"æˆåŠŸç‡: {stats['success_rate']:.2%}")
                print(f"å¹³å‡å»¶è¿Ÿ: {stats['average_latency_ms']:.2f}ms")
                
            except Exception as e:
                print(f"âŒ æ“ä½œå¤±è´¥: {e}")
                sys.exit(1)
        else:
            print("âŒ Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ Ollama:")
            print("   ollama serve")
            sys.exit(1)
