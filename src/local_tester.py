#!/usr/bin/env python3
"""
Qwen-3 æœ¬åœ°æ€§èƒ½æµ‹è¯•å¥—ä»¶

æä¾›ç®€åŒ–çš„æœ¬åœ°æ€§èƒ½æµ‹è¯•åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- åŸºç¡€ QPS æµ‹è¯•ï¼ˆå¤šçº¿ç¨‹å¹¶å‘ï¼‰
- å»¶è¿Ÿæµ‹è¯•å’Œç»Ÿè®¡åˆ†æï¼ˆP50/P95/P99ï¼‰
- æµ‹è¯•ç»“æœä¿å­˜å’Œ HTML æŠ¥å‘Šç”Ÿæˆ
- ç³»ç»Ÿèµ„æºç›‘æ§é›†æˆ

ä½œè€…: Qwen-3 éƒ¨ç½²å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
"""

import json
import time
import logging
import threading
import statistics
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
import psutil

from ollama_integration import OllamaIntegration, InferenceMetrics
from test_dataset_manager import TestDatasetManager, TestResult, EvaluationReport


@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®æ•°æ®ç±»"""
    model: str
    test_prompts: List[str]
    concurrent_users: int = 5
    test_duration: int = 60
    iterations: int = 100
    timeout: int = 30
    warmup_iterations: int = 5


@dataclass
class LatencyStats:
    """å»¶è¿Ÿç»Ÿè®¡æ•°æ®ç±»"""
    count: int
    min_ms: float
    max_ms: float
    mean_ms: float
    median_ms: float
    p95_ms: float
    p99_ms: float
    std_dev_ms: float


@dataclass
class QPSTestResult:
    """QPS æµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_id: str
    model: str
    start_time: str
    end_time: str
    duration_seconds: float
    concurrent_users: int
    total_requests: int
    successful_requests: int
    failed_requests: int
    timeout_requests: int
    qps: float
    avg_latency_ms: float
    latency_stats: LatencyStats
    error_rate: float
    throughput_tokens_per_second: float
    system_metrics: Dict[str, Any]
    errors: List[str]


@dataclass
class LatencyTestResult:
    """å»¶è¿Ÿæµ‹è¯•ç»“æœæ•°æ®ç±»"""
    test_id: str
    model: str
    start_time: str
    end_time: str
    iterations: int
    successful_iterations: int
    failed_iterations: int
    latency_stats: LatencyStats
    system_metrics: Dict[str, Any]
    errors: List[str]


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å™¨"""
    
    def __init__(self):
        self.monitoring = False
        self.metrics = []
        self.monitor_thread = None
        self.lock = threading.Lock()
    
    def start_monitoring(self, interval: float = 1.0):
        """å¼€å§‹ç›‘æ§ç³»ç»Ÿèµ„æº"""
        if self.monitoring:
            return
            
        self.monitoring = True
        self.metrics = []
        
        def monitor():
            while self.monitoring:
                try:
                    metric = {
                        'timestamp': datetime.now().isoformat(),
                        'cpu_percent': psutil.cpu_percent(interval=None),
                        'memory_percent': psutil.virtual_memory().percent,
                        'memory_used_mb': psutil.virtual_memory().used / 1024 / 1024,
                        'disk_usage_percent': psutil.disk_usage('/').percent
                    }
                    
                    with self.lock:
                        self.metrics.append(metric)
                        
                except Exception as e:
                    logging.error(f"ç³»ç»Ÿç›‘æ§é”™è¯¯: {e}")
                
                time.sleep(interval)
        
        self.monitor_thread = threading.Thread(target=monitor, daemon=True)
        self.monitor_thread.start()
    
    def stop_monitoring(self) -> Dict[str, Any]:
        """åœæ­¢ç›‘æ§å¹¶è¿”å›ç»Ÿè®¡ä¿¡æ¯"""
        self.monitoring = False
        
        if self.monitor_thread and self.monitor_thread.is_alive():
            self.monitor_thread.join(timeout=2)
        
        with self.lock:
            if not self.metrics:
                return {}
            
            cpu_values = [m['cpu_percent'] for m in self.metrics]
            memory_values = [m['memory_percent'] for m in self.metrics]
            memory_mb_values = [m['memory_used_mb'] for m in self.metrics]
            
            return {
                'samples_count': len(self.metrics),
                'duration_seconds': len(self.metrics),  # å‡è®¾æ¯ç§’ä¸€ä¸ªæ ·æœ¬
                'cpu': {
                    'avg_percent': statistics.mean(cpu_values),
                    'max_percent': max(cpu_values),
                    'min_percent': min(cpu_values)
                },
                'memory': {
                    'avg_percent': statistics.mean(memory_values),
                    'max_percent': max(memory_values),
                    'min_percent': min(memory_values),
                    'avg_used_mb': statistics.mean(memory_mb_values),
                    'max_used_mb': max(memory_mb_values)
                }
            }


class SimpleLocalTester:
    """
    ç®€åŒ–çš„æœ¬åœ°æ€§èƒ½æµ‹è¯•å™¨
    
    æä¾›åŸºç¡€çš„ QPS å’Œå»¶è¿Ÿæµ‹è¯•åŠŸèƒ½ï¼Œé€‚ç”¨äºæœ¬åœ°å¼€å‘å’Œç®€å•çš„æ€§èƒ½è¯„ä¼°ã€‚
    """
    
    def __init__(self, ollama_integration: OllamaIntegration, 
                 results_dir: str = "./test_results"):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            ollama_integration: Ollama é›†æˆå®ä¾‹
            results_dir: æµ‹è¯•ç»“æœä¿å­˜ç›®å½•
        """
        self.ollama = ollama_integration
        self.results_dir = results_dir
        self.logger = logging.getLogger(__name__)
        
        # åˆ›å»ºç»“æœç›®å½•
        os.makedirs(results_dir, exist_ok=True)
        
        # é»˜è®¤æµ‹è¯•æç¤º
        self.default_prompts = [
            "ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µã€‚",
            "å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—é˜¶ä¹˜ã€‚",
            "æè¿°ä¸€ä¸‹æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ çš„åŒºåˆ«ã€‚",
            "è¯·ç»™å‡ºä¸€äº›æé«˜ç¼–ç¨‹æ•ˆç‡çš„å»ºè®®ã€‚"
        ]
        
        # åˆå§‹åŒ–æµ‹è¯•é›†ç®¡ç†å™¨
        self.dataset_manager = TestDatasetManager()
        
        self.logger.info(f"æœ¬åœ°æµ‹è¯•å™¨åˆå§‹åŒ–å®Œæˆï¼Œç»“æœç›®å½•: {results_dir}")
    
    def _calculate_latency_stats(self, latencies: List[float]) -> LatencyStats:
        """è®¡ç®—å»¶è¿Ÿç»Ÿè®¡ä¿¡æ¯"""
        if not latencies:
            return LatencyStats(0, 0, 0, 0, 0, 0, 0, 0)
        
        sorted_latencies = sorted(latencies)
        count = len(sorted_latencies)
        
        return LatencyStats(
            count=count,
            min_ms=min(sorted_latencies),
            max_ms=max(sorted_latencies),
            mean_ms=statistics.mean(sorted_latencies),
            median_ms=statistics.median(sorted_latencies),
            p95_ms=sorted_latencies[int(count * 0.95)] if count > 0 else 0,
            p99_ms=sorted_latencies[int(count * 0.99)] if count > 0 else 0,
            std_dev_ms=statistics.stdev(sorted_latencies) if count > 1 else 0
        )
    
    def run_basic_qps_test(self, 
                          model: str, 
                          test_prompts: Optional[List[str]] = None,
                          concurrent_users: int = 5, 
                          duration: int = 60,
                          warmup_duration: int = 10) -> QPSTestResult:
        """
        è¿è¡ŒåŸºç¡€ QPS æµ‹è¯•
        
        Args:
            model: æµ‹è¯•æ¨¡å‹åç§°
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨ï¼ŒNone ä½¿ç”¨é»˜è®¤æç¤º
            concurrent_users: å¹¶å‘ç”¨æˆ·æ•°
            duration: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            warmup_duration: é¢„çƒ­æ—¶é—´ï¼ˆç§’ï¼‰
            
        Returns:
            QPSTestResult: æµ‹è¯•ç»“æœ
        """
        test_id = f"qps_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.logger.info(f"å¼€å§‹ QPS æµ‹è¯•: {test_id}")
        
        if test_prompts is None:
            test_prompts = self.default_prompts
        
        # éªŒè¯æ¨¡å‹å­˜åœ¨
        if not self.ollama.model_exists(model):
            error_msg = f"æ¨¡å‹ä¸å­˜åœ¨: {model}"
            self.logger.error(error_msg)
            return QPSTestResult(
                test_id=test_id,
                model=model,
                start_time=datetime.now().isoformat(),
                end_time=datetime.now().isoformat(),
                duration_seconds=0,
                concurrent_users=concurrent_users,
                total_requests=0,
                successful_requests=0,
                failed_requests=1,
                timeout_requests=0,
                qps=0,
                avg_latency_ms=0,
                latency_stats=LatencyStats(0, 0, 0, 0, 0, 0, 0, 0),
                error_rate=1.0,
                throughput_tokens_per_second=0,
                system_metrics={},
                errors=[error_msg]
            )
        
        # é¢„çƒ­
        if warmup_duration > 0:
            self.logger.info(f"é¢„çƒ­é˜¶æ®µå¼€å§‹ ({warmup_duration}s)...")
            self._run_warmup(model, test_prompts, min(concurrent_users, 2), warmup_duration)
            self.logger.info("é¢„çƒ­é˜¶æ®µå®Œæˆ")
        
        # å¼€å§‹ç³»ç»Ÿç›‘æ§
        monitor = SystemMonitor()
        monitor.start_monitoring()
        
        # æµ‹è¯•æ•°æ®æ”¶é›†
        results = []
        errors = []
        start_time = datetime.now()
        end_time = start_time + timedelta(seconds=duration)
        
        self.logger.info(f"QPS æµ‹è¯•å¼€å§‹: {concurrent_users} å¹¶å‘ç”¨æˆ·, {duration}s æŒç»­æ—¶é—´")
        
        def worker():
            """å·¥ä½œçº¿ç¨‹å‡½æ•°"""
            prompt_index = 0
            while datetime.now() < end_time:
                try:
                    prompt = test_prompts[prompt_index % len(test_prompts)]
                    result = self.ollama.inference_with_metrics(model, prompt)
                    results.append(result)
                    prompt_index += 1
                except Exception as e:
                    error_msg = f"å·¥ä½œçº¿ç¨‹é”™è¯¯: {e}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
        
        # å¯åŠ¨å¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker) for _ in range(concurrent_users)]
            
            # ç­‰å¾…æµ‹è¯•å®Œæˆ
            for future in as_completed(futures, timeout=duration + 30):
                try:
                    future.result()
                except Exception as e:
                    error_msg = f"çº¿ç¨‹æ‰§è¡Œé”™è¯¯: {e}"
                    errors.append(error_msg)
                    self.logger.error(error_msg)
        
        # åœæ­¢ç›‘æ§
        system_metrics = monitor.stop_monitoring()
        
        actual_end_time = datetime.now()
        actual_duration = (actual_end_time - start_time).total_seconds()
        
        # åˆ†æç»“æœ
        total_requests = len(results)
        successful_requests = len([r for r in results if r.get('status') == 'success'])
        failed_requests = len([r for r in results if r.get('status') == 'error'])
        timeout_requests = len([r for r in results if r.get('status') == 'timeout'])
        
        # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
        successful_latencies = [
            r['latency_ms'] for r in results 
            if r.get('status') == 'success' and 'latency_ms' in r
        ]
        latency_stats = self._calculate_latency_stats(successful_latencies)
        
        # è®¡ç®— QPS å’Œå…¶ä»–æŒ‡æ ‡
        qps = total_requests / actual_duration if actual_duration > 0 else 0
        error_rate = failed_requests / total_requests if total_requests > 0 else 0
        avg_latency = latency_stats.mean_ms
        
        # è®¡ç®—ååé‡ï¼ˆtokens per secondï¼‰
        total_tokens = sum(
            r.get('tokens_per_second', 0) for r in results 
            if r.get('status') == 'success'
        )
        throughput_tps = total_tokens / actual_duration if actual_duration > 0 else 0
        
        test_result = QPSTestResult(
            test_id=test_id,
            model=model,
            start_time=start_time.isoformat(),
            end_time=actual_end_time.isoformat(),
            duration_seconds=actual_duration,
            concurrent_users=concurrent_users,
            total_requests=total_requests,
            successful_requests=successful_requests,
            failed_requests=failed_requests,
            timeout_requests=timeout_requests,
            qps=qps,
            avg_latency_ms=avg_latency,
            latency_stats=latency_stats,
            error_rate=error_rate,
            throughput_tokens_per_second=throughput_tps,
            system_metrics=system_metrics,
            errors=errors
        )
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_qps_test_results(test_result, results)
        
        self.logger.info(f"QPS æµ‹è¯•å®Œæˆ: QPS={qps:.2f}, å¹³å‡å»¶è¿Ÿ={avg_latency:.2f}ms, "
                        f"é”™è¯¯ç‡={error_rate:.2%}")
        
        return test_result
    
    def run_latency_test(self, 
                        model: str, 
                        test_prompts: Optional[List[str]] = None,
                        iterations: int = 100,
                        warmup_iterations: int = 5) -> LatencyTestResult:
        """
        è¿è¡Œå»¶è¿Ÿæµ‹è¯•
        
        Args:
            model: æµ‹è¯•æ¨¡å‹åç§°
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨ï¼ŒNone ä½¿ç”¨é»˜è®¤æç¤º
            iterations: æµ‹è¯•è¿­ä»£æ¬¡æ•°
            warmup_iterations: é¢„çƒ­è¿­ä»£æ¬¡æ•°
            
        Returns:
            LatencyTestResult: æµ‹è¯•ç»“æœ
        """
        test_id = f"latency_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.logger.info(f"å¼€å§‹å»¶è¿Ÿæµ‹è¯•: {test_id}")
        
        if test_prompts is None:
            test_prompts = self.default_prompts
        
        # éªŒè¯æ¨¡å‹å­˜åœ¨
        if not self.ollama.model_exists(model):
            error_msg = f"æ¨¡å‹ä¸å­˜åœ¨: {model}"
            self.logger.error(error_msg)
            return LatencyTestResult(
                test_id=test_id,
                model=model,
                start_time=datetime.now().isoformat(),
                end_time=datetime.now().isoformat(),
                iterations=0,
                successful_iterations=0,
                failed_iterations=1,
                latency_stats=LatencyStats(0, 0, 0, 0, 0, 0, 0, 0),
                system_metrics={},
                errors=[error_msg]
            )
        
        # é¢„çƒ­
        if warmup_iterations > 0:
            self.logger.info(f"é¢„çƒ­é˜¶æ®µå¼€å§‹ ({warmup_iterations} æ¬¡è¿­ä»£)...")
            for i in range(warmup_iterations):
                prompt = test_prompts[i % len(test_prompts)]
                self.ollama.inference_with_metrics(model, prompt)
            self.logger.info("é¢„çƒ­é˜¶æ®µå®Œæˆ")
        
        # å¼€å§‹ç³»ç»Ÿç›‘æ§
        monitor = SystemMonitor()
        monitor.start_monitoring()
        
        start_time = datetime.now()
        results = []
        errors = []
        
        self.logger.info(f"å»¶è¿Ÿæµ‹è¯•å¼€å§‹: {iterations} æ¬¡è¿­ä»£")
        
        for i in range(iterations):
            try:
                prompt = test_prompts[i % len(test_prompts)]
                result = self.ollama.inference_with_metrics(model, prompt)
                results.append(result)
                
                # è¿›åº¦æŠ¥å‘Š
                if (i + 1) % max(1, iterations // 10) == 0:
                    progress = (i + 1) / iterations * 100
                    self.logger.info(f"å»¶è¿Ÿæµ‹è¯•è¿›åº¦: {progress:.1f}% ({i + 1}/{iterations})")
                    
            except Exception as e:
                error_msg = f"è¿­ä»£ {i+1} é”™è¯¯: {e}"
                errors.append(error_msg)
                self.logger.error(error_msg)
        
        end_time = datetime.now()
        
        # åœæ­¢ç›‘æ§
        system_metrics = monitor.stop_monitoring()
        
        # åˆ†æç»“æœ
        successful_results = [r for r in results if r.get('status') == 'success']
        failed_results = [r for r in results if r.get('status') != 'success']
        
        successful_latencies = [
            r['latency_ms'] for r in successful_results 
            if 'latency_ms' in r
        ]
        
        latency_stats = self._calculate_latency_stats(successful_latencies)
        
        test_result = LatencyTestResult(
            test_id=test_id,
            model=model,
            start_time=start_time.isoformat(),
            end_time=end_time.isoformat(),
            iterations=iterations,
            successful_iterations=len(successful_results),
            failed_iterations=len(failed_results),
            latency_stats=latency_stats,
            system_metrics=system_metrics,
            errors=errors
        )
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        self._save_latency_test_results(test_result, results)
        
        self.logger.info(f"å»¶è¿Ÿæµ‹è¯•å®Œæˆ: å¹³å‡å»¶è¿Ÿ={latency_stats.mean_ms:.2f}ms, "
                        f"P95={latency_stats.p95_ms:.2f}ms, P99={latency_stats.p99_ms:.2f}ms")
        
        return test_result
    
    def _run_warmup(self, model: str, test_prompts: List[str], 
                   concurrent_users: int, duration: int):
        """æ‰§è¡Œé¢„çƒ­"""
        end_time = datetime.now() + timedelta(seconds=duration)
        
        def warmup_worker():
            prompt_index = 0
            while datetime.now() < end_time:
                try:
                    prompt = test_prompts[prompt_index % len(test_prompts)]
                    self.ollama.inference_with_metrics(model, prompt)
                    prompt_index += 1
                except Exception:
                    pass  # é¢„çƒ­é˜¶æ®µå¿½ç•¥é”™è¯¯
        
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(warmup_worker) for _ in range(concurrent_users)]
            for future in as_completed(futures, timeout=duration + 10):
                try:
                    future.result()
                except Exception:
                    pass  # é¢„çƒ­é˜¶æ®µå¿½ç•¥é”™è¯¯
    
    def _save_qps_test_results(self, test_result: QPSTestResult, 
                              detailed_results: List[Dict[str, Any]]):
        """ä¿å­˜ QPS æµ‹è¯•ç»“æœ"""
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = os.path.join(self.results_dir, f"{test_result.test_id}_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(test_result), f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        details_file = os.path.join(self.results_dir, f"{test_result.test_id}_details.jsonl")
        with open(details_file, 'w', encoding='utf-8') as f:
            for result in detailed_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        self.logger.info(f"QPS æµ‹è¯•ç»“æœå·²ä¿å­˜: {summary_file}")
    
    def _save_latency_test_results(self, test_result: LatencyTestResult, 
                                  detailed_results: List[Dict[str, Any]]):
        """ä¿å­˜å»¶è¿Ÿæµ‹è¯•ç»“æœ"""
        # ä¿å­˜æ±‡æ€»ç»“æœ
        summary_file = os.path.join(self.results_dir, f"{test_result.test_id}_summary.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(asdict(test_result), f, indent=2, ensure_ascii=False)
        
        # ä¿å­˜è¯¦ç»†ç»“æœ
        details_file = os.path.join(self.results_dir, f"{test_result.test_id}_details.jsonl")
        with open(details_file, 'w', encoding='utf-8') as f:
            for result in detailed_results:
                f.write(json.dumps(result, ensure_ascii=False) + '\n')
        
        self.logger.info(f"å»¶è¿Ÿæµ‹è¯•ç»“æœå·²ä¿å­˜: {summary_file}")
    
    def generate_simple_html_report(self, test_result) -> str:
        """
        ç”Ÿæˆç®€å•çš„ HTML æŠ¥å‘Š
        
        Args:
            test_result: æµ‹è¯•ç»“æœï¼ˆQPSTestResult æˆ– LatencyTestResultï¼‰
            
        Returns:
            str: ç”Ÿæˆçš„ HTML æ–‡ä»¶è·¯å¾„
        """
        if isinstance(test_result, QPSTestResult):
            return self._generate_qps_html_report(test_result)
        elif isinstance(test_result, LatencyTestResult):
            return self._generate_latency_html_report(test_result)
        else:
            raise ValueError("ä¸æ”¯æŒçš„æµ‹è¯•ç»“æœç±»å‹")
    
    def _generate_qps_html_report(self, test_result: QPSTestResult) -> str:
        """ç”Ÿæˆ QPS æµ‹è¯•çš„ HTML æŠ¥å‘Š"""
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Qwen-3 QPS æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                    margin: 20px; 
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }}
                h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
                h2 {{ color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                .metric-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }}
                .metric {{ 
                    padding: 15px; 
                    background: #f8f9fa; 
                    border-radius: 8px;
                    border-left: 4px solid #3498db;
                }}
                .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 5px; }}
                .metric-value {{ font-size: 1.2em; color: #27ae60; }}
                .good {{ color: #27ae60; }}
                .warning {{ color: #f39c12; }}
                .error {{ color: #e74c3c; }}
                .status-badge {{
                    display: inline-block;
                    padding: 4px 12px;
                    border-radius: 20px;
                    font-size: 0.9em;
                    font-weight: bold;
                }}
                .status-success {{ background: #d4edda; color: #155724; }}
                .status-warning {{ background: #fff3cd; color: #856404; }}
                .status-error {{ background: #f8d7da; color: #721c24; }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                th, td {{
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{ background-color: #3498db; color: white; }}
                .footer {{
                    margin-top: 30px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    color: #7f8c8d;
                    text-align: center;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>ğŸš€ Qwen-3 QPS æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
                
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•æ¨¡å‹</div>
                        <div class="metric-value">{model}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯• ID</div>
                        <div class="metric-value">{test_id}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•æ—¶é—´</div>
                        <div class="metric-value">{duration:.1f}s</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">å¹¶å‘ç”¨æˆ·</div>
                        <div class="metric-value">{concurrent_users}</div>
                    </div>
                </div>

                <h2>ğŸ“Š æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡</h2>
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">QPS (æ¯ç§’æŸ¥è¯¢æ•°)</div>
                        <div class="metric-value {qps_class}">{qps:.2f}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">å¹³å‡å»¶è¿Ÿ</div>
                        <div class="metric-value {latency_class}">{avg_latency:.2f}ms</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">é”™è¯¯ç‡</div>
                        <div class="metric-value {error_class}">{error_rate:.2%}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">ååé‡</div>
                        <div class="metric-value">{throughput:.2f} tokens/s</div>
                    </div>
                </div>

                <h2>ğŸ“ˆ å»¶è¿Ÿåˆ†å¸ƒç»Ÿè®¡</h2>
                <table>
                    <tr>
                        <th>æŒ‡æ ‡</th>
                        <th>å€¼ (ms)</th>
                        <th>è¯´æ˜</th>
                    </tr>
                    <tr>
                        <td>æœ€å°å»¶è¿Ÿ</td>
                        <td>{min_latency:.2f}</td>
                        <td>æœ€å¿«å“åº”æ—¶é—´</td>
                    </tr>
                    <tr>
                        <td>æœ€å¤§å»¶è¿Ÿ</td>
                        <td>{max_latency:.2f}</td>
                        <td>æœ€æ…¢å“åº”æ—¶é—´</td>
                    </tr>
                    <tr>
                        <td>ä¸­ä½æ•° (P50)</td>
                        <td>{p50_latency:.2f}</td>
                        <td>50% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>95åˆ†ä½æ•° (P95)</td>
                        <td>{p95_latency:.2f}</td>
                        <td>95% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>99åˆ†ä½æ•° (P99)</td>
                        <td>{p99_latency:.2f}</td>
                        <td>99% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>æ ‡å‡†å·®</td>
                        <td>{std_dev:.2f}</td>
                        <td>å»¶è¿Ÿåˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦</td>
                    </tr>
                </table>

                <h2>ğŸ’» ç³»ç»Ÿèµ„æºä½¿ç”¨</h2>
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">å¹³å‡ CPU ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{cpu_avg:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æœ€å¤§ CPU ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{cpu_max:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">å¹³å‡å†…å­˜ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{memory_avg:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æœ€å¤§å†…å­˜ä½¿ç”¨</div>
                        <div class="metric-value">{memory_max:.1f} MB</div>
                    </div>
                </div>

                <h2>ğŸ“‹ æµ‹è¯•è¯¦æƒ…</h2>
                <table>
                    <tr>
                        <th>æŒ‡æ ‡</th>
                        <th>æ•°å€¼</th>
                    </tr>
                    <tr>
                        <td>æ€»è¯·æ±‚æ•°</td>
                        <td>{total_requests}</td>
                    </tr>
                    <tr>
                        <td>æˆåŠŸè¯·æ±‚æ•°</td>
                        <td><span class="good">{successful_requests}</span></td>
                    </tr>
                    <tr>
                        <td>å¤±è´¥è¯·æ±‚æ•°</td>
                        <td><span class="error">{failed_requests}</span></td>
                    </tr>
                    <tr>
                        <td>è¶…æ—¶è¯·æ±‚æ•°</td>
                        <td><span class="warning">{timeout_requests}</span></td>
                    </tr>
                    <tr>
                        <td>å¼€å§‹æ—¶é—´</td>
                        <td>{start_time}</td>
                    </tr>
                    <tr>
                        <td>ç»“æŸæ—¶é—´</td>
                        <td>{end_time}</td>
                    </tr>
                </table>

                <div class="footer">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}</p>
                    <p>ğŸ¤– ç”± Qwen-3 æœ¬åœ°æ€§èƒ½æµ‹è¯•å¥—ä»¶ç”Ÿæˆ</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # ç¡®å®šæ€§èƒ½æŒ‡æ ‡çš„æ ·å¼ç±»
        qps_class = "good" if test_result.qps > 10 else "warning" if test_result.qps > 5 else "error"
        latency_class = "good" if test_result.avg_latency_ms < 1000 else "warning" if test_result.avg_latency_ms < 3000 else "error"
        error_class = "good" if test_result.error_rate < 0.01 else "warning" if test_result.error_rate < 0.05 else "error"
        
        # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        cpu_avg = test_result.system_metrics.get('cpu', {}).get('avg_percent', 0)
        cpu_max = test_result.system_metrics.get('cpu', {}).get('max_percent', 0)
        memory_avg = test_result.system_metrics.get('memory', {}).get('avg_percent', 0)
        memory_max = test_result.system_metrics.get('memory', {}).get('max_used_mb', 0)
        
        html_content = html_template.format(
            model=test_result.model,
            test_id=test_result.test_id,
            duration=test_result.duration_seconds,
            concurrent_users=test_result.concurrent_users,
            qps=test_result.qps,
            qps_class=qps_class,
            avg_latency=test_result.avg_latency_ms,
            latency_class=latency_class,
            error_rate=test_result.error_rate,
            error_class=error_class,
            throughput=test_result.throughput_tokens_per_second,
            min_latency=test_result.latency_stats.min_ms,
            max_latency=test_result.latency_stats.max_ms,
            p50_latency=test_result.latency_stats.median_ms,
            p95_latency=test_result.latency_stats.p95_ms,
            p99_latency=test_result.latency_stats.p99_ms,
            std_dev=test_result.latency_stats.std_dev_ms,
            cpu_avg=cpu_avg,
            cpu_max=cpu_max,
            memory_avg=memory_avg,
            memory_max=memory_max,
            total_requests=test_result.total_requests,
            successful_requests=test_result.successful_requests,
            failed_requests=test_result.failed_requests,
            timeout_requests=test_result.timeout_requests,
            start_time=test_result.start_time,
            end_time=test_result.end_time,
            report_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        report_file = os.path.join(self.results_dir, f"{test_result.test_id}_report.html")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"QPS æµ‹è¯• HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
    
    def _generate_latency_html_report(self, test_result: LatencyTestResult) -> str:
        """ç”Ÿæˆå»¶è¿Ÿæµ‹è¯•çš„ HTML æŠ¥å‘Š"""
        html_template = """
        <!DOCTYPE html>
        <html lang="zh-CN">
        <head>
            <meta charset="UTF-8">
            <meta name="viewport" content="width=device-width, initial-scale=1.0">
            <title>Qwen-3 å»¶è¿Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ 
                    font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Arial, sans-serif; 
                    margin: 20px; 
                    background-color: #f5f5f5;
                }}
                .container {{
                    max-width: 1200px;
                    margin: 0 auto;
                    background: white;
                    padding: 30px;
                    border-radius: 10px;
                    box-shadow: 0 2px 10px rgba(0,0,0,0.1);
                }}
                h1 {{ color: #2c3e50; text-align: center; margin-bottom: 30px; }}
                h2 {{ color: #34495e; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}
                .metric-grid {{
                    display: grid;
                    grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                    gap: 20px;
                    margin: 20px 0;
                }}
                .metric {{ 
                    padding: 15px; 
                    background: #f8f9fa; 
                    border-radius: 8px;
                    border-left: 4px solid #3498db;
                }}
                .metric-title {{ font-weight: bold; color: #2c3e50; margin-bottom: 5px; }}
                .metric-value {{ font-size: 1.2em; color: #27ae60; }}
                .good {{ color: #27ae60; }}
                .warning {{ color: #f39c12; }}
                .error {{ color: #e74c3c; }}
                table {{
                    width: 100%;
                    border-collapse: collapse;
                    margin: 20px 0;
                }}
                th, td {{
                    padding: 12px;
                    text-align: left;
                    border-bottom: 1px solid #ddd;
                }}
                th {{ background-color: #3498db; color: white; }}
                .footer {{
                    margin-top: 30px;
                    padding-top: 20px;
                    border-top: 1px solid #ddd;
                    color: #7f8c8d;
                    text-align: center;
                }}
            </style>
        </head>
        <body>
            <div class="container">
                <h1>âš¡ Qwen-3 å»¶è¿Ÿæ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
                
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•æ¨¡å‹</div>
                        <div class="metric-value">{model}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯• ID</div>
                        <div class="metric-value">{test_id}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æµ‹è¯•è¿­ä»£æ¬¡æ•°</div>
                        <div class="metric-value">{iterations}</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æˆåŠŸç‡</div>
                        <div class="metric-value {success_class}">{success_rate:.2%}</div>
                    </div>
                </div>

                <h2>ğŸ“Š å»¶è¿Ÿç»Ÿè®¡åˆ†æ</h2>
                <table>
                    <tr>
                        <th>æŒ‡æ ‡</th>
                        <th>å€¼ (ms)</th>
                        <th>è¯´æ˜</th>
                    </tr>
                    <tr>
                        <td>å¹³å‡å»¶è¿Ÿ</td>
                        <td class="{avg_class}">{mean_latency:.2f}</td>
                        <td>æ‰€æœ‰æˆåŠŸè¯·æ±‚çš„å¹³å‡å“åº”æ—¶é—´</td>
                    </tr>
                    <tr>
                        <td>ä¸­ä½æ•°å»¶è¿Ÿ (P50)</td>
                        <td>{median_latency:.2f}</td>
                        <td>50% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>95åˆ†ä½æ•° (P95)</td>
                        <td class="{p95_class}">{p95_latency:.2f}</td>
                        <td>95% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>99åˆ†ä½æ•° (P99)</td>
                        <td class="{p99_class}">{p99_latency:.2f}</td>
                        <td>99% çš„è¯·æ±‚å»¶è¿Ÿä½äºæ­¤å€¼</td>
                    </tr>
                    <tr>
                        <td>æœ€å°å»¶è¿Ÿ</td>
                        <td class="good">{min_latency:.2f}</td>
                        <td>æœ€å¿«å“åº”æ—¶é—´</td>
                    </tr>
                    <tr>
                        <td>æœ€å¤§å»¶è¿Ÿ</td>
                        <td class="{max_class}">{max_latency:.2f}</td>
                        <td>æœ€æ…¢å“åº”æ—¶é—´</td>
                    </tr>
                    <tr>
                        <td>æ ‡å‡†å·®</td>
                        <td>{std_dev:.2f}</td>
                        <td>å»¶è¿Ÿåˆ†å¸ƒçš„ç¦»æ•£ç¨‹åº¦</td>
                    </tr>
                </table>

                <h2>ğŸ’» ç³»ç»Ÿèµ„æºä½¿ç”¨</h2>
                <div class="metric-grid">
                    <div class="metric">
                        <div class="metric-title">å¹³å‡ CPU ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{cpu_avg:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æœ€å¤§ CPU ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{cpu_max:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">å¹³å‡å†…å­˜ä½¿ç”¨ç‡</div>
                        <div class="metric-value">{memory_avg:.1f}%</div>
                    </div>
                    <div class="metric">
                        <div class="metric-title">æœ€å¤§å†…å­˜ä½¿ç”¨</div>
                        <div class="metric-value">{memory_max:.1f} MB</div>
                    </div>
                </div>

                <h2>ğŸ“‹ æµ‹è¯•è¯¦æƒ…</h2>
                <table>
                    <tr>
                        <th>æŒ‡æ ‡</th>
                        <th>æ•°å€¼</th>
                    </tr>
                    <tr>
                        <td>æ€»è¿­ä»£æ¬¡æ•°</td>
                        <td>{iterations}</td>
                    </tr>
                    <tr>
                        <td>æˆåŠŸè¿­ä»£æ¬¡æ•°</td>
                        <td><span class="good">{successful_iterations}</span></td>
                    </tr>
                    <tr>
                        <td>å¤±è´¥è¿­ä»£æ¬¡æ•°</td>
                        <td><span class="error">{failed_iterations}</span></td>
                    </tr>
                    <tr>
                        <td>å¼€å§‹æ—¶é—´</td>
                        <td>{start_time}</td>
                    </tr>
                    <tr>
                        <td>ç»“æŸæ—¶é—´</td>
                        <td>{end_time}</td>
                    </tr>
                </table>

                <div class="footer">
                    <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {report_time}</p>
                    <p>ğŸ¤– ç”± Qwen-3 æœ¬åœ°æ€§èƒ½æµ‹è¯•å¥—ä»¶ç”Ÿæˆ</p>
                </div>
            </div>
        </body>
        </html>
        """
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = (test_result.successful_iterations / test_result.iterations 
                       if test_result.iterations > 0 else 0)
        
        # ç¡®å®šæ€§èƒ½æŒ‡æ ‡çš„æ ·å¼ç±»
        success_class = "good" if success_rate > 0.95 else "warning" if success_rate > 0.9 else "error"
        avg_class = "good" if test_result.latency_stats.mean_ms < 1000 else "warning" if test_result.latency_stats.mean_ms < 3000 else "error"
        p95_class = "good" if test_result.latency_stats.p95_ms < 2000 else "warning" if test_result.latency_stats.p95_ms < 5000 else "error"
        p99_class = "good" if test_result.latency_stats.p99_ms < 3000 else "warning" if test_result.latency_stats.p99_ms < 8000 else "error"
        max_class = "good" if test_result.latency_stats.max_ms < 5000 else "warning" if test_result.latency_stats.max_ms < 10000 else "error"
        
        # ç³»ç»Ÿèµ„æºæŒ‡æ ‡
        cpu_avg = test_result.system_metrics.get('cpu', {}).get('avg_percent', 0)
        cpu_max = test_result.system_metrics.get('cpu', {}).get('max_percent', 0)
        memory_avg = test_result.system_metrics.get('memory', {}).get('avg_percent', 0)
        memory_max = test_result.system_metrics.get('memory', {}).get('max_used_mb', 0)
        
        html_content = html_template.format(
            model=test_result.model,
            test_id=test_result.test_id,
            iterations=test_result.iterations,
            success_rate=success_rate,
            success_class=success_class,
            mean_latency=test_result.latency_stats.mean_ms,
            avg_class=avg_class,
            median_latency=test_result.latency_stats.median_ms,
            p95_latency=test_result.latency_stats.p95_ms,
            p95_class=p95_class,
            p99_latency=test_result.latency_stats.p99_ms,
            p99_class=p99_class,
            min_latency=test_result.latency_stats.min_ms,
            max_latency=test_result.latency_stats.max_ms,
            max_class=max_class,
            std_dev=test_result.latency_stats.std_dev_ms,
            cpu_avg=cpu_avg,
            cpu_max=cpu_max,
            memory_avg=memory_avg,
            memory_max=memory_max,
            successful_iterations=test_result.successful_iterations,
            failed_iterations=test_result.failed_iterations,
            start_time=test_result.start_time,
            end_time=test_result.end_time,
            report_time=datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        )
        
        report_file = os.path.join(self.results_dir, f"{test_result.test_id}_report.html")
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(html_content)
        
        self.logger.info(f"å»¶è¿Ÿæµ‹è¯• HTML æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        return report_file
    
    def run_comprehensive_test(self, model: str, 
                             test_prompts: Optional[List[str]] = None) -> Dict[str, Any]:
        """
        è¿è¡Œç»¼åˆæ€§èƒ½æµ‹è¯•
        
        Args:
            model: æµ‹è¯•æ¨¡å‹åç§°
            test_prompts: æµ‹è¯•æç¤ºåˆ—è¡¨
            
        Returns:
            Dict: åŒ…å«æ‰€æœ‰æµ‹è¯•ç»“æœçš„å­—å…¸
        """
        self.logger.info(f"å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•: {model}")
        
        results = {}
        
        try:
            # å»¶è¿Ÿæµ‹è¯•
            self.logger.info("ç¬¬ 1/2 æ­¥: æ‰§è¡Œå»¶è¿Ÿæµ‹è¯•...")
            latency_result = self.run_latency_test(model, test_prompts, iterations=50)
            results['latency_test'] = latency_result
            latency_report = self.generate_simple_html_report(latency_result)
            results['latency_report'] = latency_report
            
            # QPS æµ‹è¯•
            self.logger.info("ç¬¬ 2/2 æ­¥: æ‰§è¡Œ QPS æµ‹è¯•...")
            qps_result = self.run_basic_qps_test(model, test_prompts, 
                                               concurrent_users=3, duration=30)
            results['qps_test'] = qps_result
            qps_report = self.generate_simple_html_report(qps_result)
            results['qps_report'] = qps_report
            
            # ç”Ÿæˆç»¼åˆæ‘˜è¦
            summary = {
                'model': model,
                'test_time': datetime.now().isoformat(),
                'latency_summary': {
                    'avg_ms': latency_result.latency_stats.mean_ms,
                    'p95_ms': latency_result.latency_stats.p95_ms,
                    'p99_ms': latency_result.latency_stats.p99_ms,
                    'success_rate': (latency_result.successful_iterations / 
                                   latency_result.iterations if latency_result.iterations > 0 else 0)
                },
                'qps_summary': {
                    'qps': qps_result.qps,
                    'avg_latency_ms': qps_result.avg_latency_ms,
                    'error_rate': qps_result.error_rate,
                    'throughput_tps': qps_result.throughput_tokens_per_second
                }
            }
            
            results['comprehensive_summary'] = summary
            
            # ä¿å­˜ç»¼åˆæ‘˜è¦
            summary_file = os.path.join(self.results_dir, 
                                      f"comprehensive_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(summary_file, 'w', encoding='utf-8') as f:
                json.dump(summary, f, indent=2, ensure_ascii=False)
            
            results['summary_file'] = summary_file
            
            self.logger.info(f"ç»¼åˆæ€§èƒ½æµ‹è¯•å®Œæˆ: {model}")
            
        except Exception as e:
            error_msg = f"ç»¼åˆæµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}"
            self.logger.error(error_msg)
            results['error'] = error_msg
        
        return results
    
    def run_dataset_evaluation(self, model: str, dataset_name: str,
                             sample_count: Optional[int] = None,
                             categories: Optional[List[str]] = None) -> EvaluationReport:
        """
        è¿è¡Œæµ‹è¯•é›†è¯„ä¼°
        
        Args:
            model: æµ‹è¯•æ¨¡å‹åç§°
            dataset_name: æµ‹è¯•é›†åç§°
            sample_count: æ ·æœ¬æ•°é‡é™åˆ¶
            categories: ç­›é€‰ç‰¹å®šç±»åˆ«
            
        Returns:
            EvaluationReport: è¯„ä¼°æŠ¥å‘Š
        """
        self.logger.info(f"å¼€å§‹æµ‹è¯•é›†è¯„ä¼°: {model} on {dataset_name}")
        
        # éªŒè¯æ¨¡å‹å­˜åœ¨
        if not self.ollama.model_exists(model):
            raise ValueError(f"æ¨¡å‹ä¸å­˜åœ¨: {model}")
        
        # è·å–æµ‹è¯•æ ·æœ¬
        test_samples = self.dataset_manager.get_test_samples(
            dataset_name, sample_count, categories
        )
        
        if not test_samples:
            raise ValueError(f"æœªæ‰¾åˆ°æµ‹è¯•æ ·æœ¬: {dataset_name}")
        
        self.logger.info(f"è·å–åˆ° {len(test_samples)} ä¸ªæµ‹è¯•æ ·æœ¬")
        
        # åˆ›å»ºæµ‹è¯•æç¤ºè¯
        prompts = self.dataset_manager.create_test_prompts(dataset_name, test_samples)
        
        # æ‰§è¡Œæ¨¡å‹æ¨ç†
        model_responses = []
        for i, prompt in enumerate(prompts):
            try:
                self.logger.info(f"å¤„ç†æ ·æœ¬ {i+1}/{len(prompts)}")
                response = self.ollama.inference_with_metrics(model, prompt)
                model_responses.append(response)
            except Exception as e:
                self.logger.error(f"æ ·æœ¬ {i+1} æ¨ç†å¤±è´¥: {e}")
                model_responses.append({
                    'response': '',
                    'latency_ms': 0,
                    'status': 'error',
                    'error': str(e)
                })
        
        # è¯„ä¼°ç»“æœ
        test_results = self.dataset_manager.evaluate_model_responses(
            dataset_name, test_samples, model_responses
        )
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report = self.dataset_manager.generate_evaluation_report(
            dataset_name, model, test_results
        )
        
        # ä¿å­˜æŠ¥å‘Š
        json_report_path = self.dataset_manager.save_evaluation_report(
            report, self.results_dir
        )
        html_report_path = self.dataset_manager.generate_html_report(
            report, self.results_dir
        )
        
        self.logger.info(f"æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
        self.logger.info(f"JSONæŠ¥å‘Š: {json_report_path}")
        self.logger.info(f"HTMLæŠ¥å‘Š: {html_report_path}")
        
        return report
    
    def run_all_dataset_evaluations(self, model: str,
                                   sample_count: Optional[int] = None) -> Dict[str, EvaluationReport]:
        """
        è¿è¡Œæ‰€æœ‰å¯ç”¨æµ‹è¯•é›†çš„è¯„ä¼°
        
        Args:
            model: æµ‹è¯•æ¨¡å‹åç§°
            sample_count: æ¯ä¸ªæµ‹è¯•é›†çš„æ ·æœ¬æ•°é‡é™åˆ¶
            
        Returns:
            Dict[str, EvaluationReport]: æ‰€æœ‰æµ‹è¯•é›†çš„è¯„ä¼°æŠ¥å‘Š
        """
        self.logger.info(f"å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•é›†è¯„ä¼°: {model}")
        
        available_datasets = self.dataset_manager.list_available_datasets()
        reports = {}
        
        for dataset_name in available_datasets:
            try:
                self.logger.info(f"è¯„ä¼°æµ‹è¯•é›†: {dataset_name}")
                report = self.run_dataset_evaluation(model, dataset_name, sample_count)
                reports[dataset_name] = report
                
                # æ˜¾ç¤ºç®€è¦ç»“æœ
                self.logger.info(f"  - æ€»æ ·æœ¬æ•°: {report.total_samples}")
                self.logger.info(f"  - æˆåŠŸç‡: {report.successful_tests/report.total_samples:.2%}")
                self.logger.info(f"  - è¯„åˆ†å‡†ç¡®æ€§: {report.avg_score_accuracy:.2%}")
                self.logger.info(f"  - ç±»åˆ«å‡†ç¡®æ€§: {report.category_accuracy:.2%}")
                
            except Exception as e:
                self.logger.error(f"æµ‹è¯•é›† {dataset_name} è¯„ä¼°å¤±è´¥: {e}")
                continue
        
        # ç”Ÿæˆç»¼åˆæ‘˜è¦
        summary = {
            'model': model,
            'evaluation_time': datetime.now().isoformat(),
            'datasets_evaluated': len(reports),
            'total_samples': sum(r.total_samples for r in reports.values()),
            'overall_success_rate': sum(r.successful_tests for r in reports.values()) / sum(r.total_samples for r in reports.values()) if reports else 0,
            'average_score_accuracy': sum(r.avg_score_accuracy for r in reports.values()) / len(reports) if reports else 0,
            'average_category_accuracy': sum(r.category_accuracy for r in reports.values()) / len(reports) if reports else 0,
            'dataset_summaries': {
                name: {
                    'total_samples': report.total_samples,
                    'success_rate': report.successful_tests / report.total_samples if report.total_samples > 0 else 0,
                    'score_accuracy': report.avg_score_accuracy,
                    'category_accuracy': report.category_accuracy
                }
                for name, report in reports.items()
            }
        }
        
        # ä¿å­˜ç»¼åˆæ‘˜è¦
        summary_file = os.path.join(self.results_dir, 
                                  f"all_datasets_{model.replace(':', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"æ‰€æœ‰æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
        self.logger.info(f"ç»¼åˆæ‘˜è¦: {summary_file}")
        self.logger.info(f"è¯„ä¼°äº† {len(reports)} ä¸ªæµ‹è¯•é›†ï¼Œå…± {summary['total_samples']} ä¸ªæ ·æœ¬")
        self.logger.info(f"æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
        self.logger.info(f"å¹³å‡è¯„åˆ†å‡†ç¡®æ€§: {summary['average_score_accuracy']:.2%}")
        self.logger.info(f"å¹³å‡ç±»åˆ«å‡†ç¡®æ€§: {summary['average_category_accuracy']:.2%}")
        
        return reports
    
    def list_available_datasets(self) -> List[str]:
        """åˆ—å‡ºå¯ç”¨çš„æµ‹è¯•é›†"""
        return self.dataset_manager.list_available_datasets()
    
    def get_dataset_info(self, dataset_name: str) -> Optional[Dict[str, Any]]:
        """è·å–æµ‹è¯•é›†ä¿¡æ¯"""
        return self.dataset_manager.get_dataset_info(dataset_name)


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•å’Œæµ‹è¯•
    import sys
    import argparse
    from .ollama_integration import create_ollama_client
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description='Qwen-3 æœ¬åœ°æ€§èƒ½æµ‹è¯•å·¥å…·')
    parser.add_argument('--model', default='qwen3:0.6b', help='æµ‹è¯•æ¨¡å‹åç§°')
    parser.add_argument('--test-type', choices=['latency', 'qps', 'comprehensive', 'dataset', 'all-datasets'], 
                       default='comprehensive', help='æµ‹è¯•ç±»å‹')
    parser.add_argument('--concurrent-users', type=int, default=5, help='QPS æµ‹è¯•å¹¶å‘ç”¨æˆ·æ•°')
    parser.add_argument('--duration', type=int, default=60, help='QPS æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰')
    parser.add_argument('--iterations', type=int, default=100, help='å»¶è¿Ÿæµ‹è¯•è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--ollama-url', default='http://localhost:11434', help='Ollama æœåŠ¡åœ°å€')
    parser.add_argument('--dataset', help='æŒ‡å®šæµ‹è¯•é›†åç§°ï¼ˆç”¨äºdatasetæµ‹è¯•ç±»å‹ï¼‰')
    parser.add_argument('--sample-count', type=int, help='æµ‹è¯•æ ·æœ¬æ•°é‡é™åˆ¶')
    parser.add_argument('--categories', nargs='+', help='ç­›é€‰ç‰¹å®šç±»åˆ«')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»º Ollama å®¢æˆ·ç«¯å’Œæµ‹è¯•å™¨
        with create_ollama_client(args.ollama_url) as ollama_client:
            tester = SimpleLocalTester(ollama_client)
            
            print(f"ğŸ” æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€...")
            if not ollama_client.check_ollama_status():
                print("âŒ Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ Ollama")
                sys.exit(1)
            
            print("âœ… Ollama æœåŠ¡æ­£å¸¸")
            
            # æ£€æŸ¥æ¨¡å‹å­˜åœ¨æ€§
            if not ollama_client.model_exists(args.model):
                print(f"âŒ æ¨¡å‹ä¸å­˜åœ¨: {args.model}")
                print("å¯ç”¨æ¨¡å‹:")
                for model in ollama_client.list_models():
                    print(f"  - {model}")
                sys.exit(1)
            
            print(f"âœ… æ¨¡å‹å­˜åœ¨: {args.model}")
            
            # æ‰§è¡Œæµ‹è¯•
            if args.test_type == 'latency':
                print(f"ğŸ“Š å¼€å§‹å»¶è¿Ÿæµ‹è¯• ({args.iterations} æ¬¡è¿­ä»£)...")
                result = tester.run_latency_test(args.model, iterations=args.iterations)
                report = tester.generate_simple_html_report(result)
                print(f"âœ… å»¶è¿Ÿæµ‹è¯•å®Œæˆ")
                print(f"   å¹³å‡å»¶è¿Ÿ: {result.latency_stats.mean_ms:.2f}ms")
                print(f"   P95 å»¶è¿Ÿ: {result.latency_stats.p95_ms:.2f}ms")
                print(f"   P99 å»¶è¿Ÿ: {result.latency_stats.p99_ms:.2f}ms")
                print(f"   æŠ¥å‘Šæ–‡ä»¶: {report}")
                
            elif args.test_type == 'qps':
                print(f"ğŸ“Š å¼€å§‹ QPS æµ‹è¯• ({args.concurrent_users} å¹¶å‘, {args.duration}s)...")
                result = tester.run_basic_qps_test(
                    args.model, 
                    concurrent_users=args.concurrent_users,
                    duration=args.duration
                )
                report = tester.generate_simple_html_report(result)
                print(f"âœ… QPS æµ‹è¯•å®Œæˆ")
                print(f"   QPS: {result.qps:.2f}")
                print(f"   å¹³å‡å»¶è¿Ÿ: {result.avg_latency_ms:.2f}ms")
                print(f"   é”™è¯¯ç‡: {result.error_rate:.2%}")
                print(f"   æŠ¥å‘Šæ–‡ä»¶: {report}")
                
            elif args.test_type == 'comprehensive':
                print(f"ğŸ“Š å¼€å§‹ç»¼åˆæ€§èƒ½æµ‹è¯•...")
                results = tester.run_comprehensive_test(args.model)
                
                if 'error' in results:
                    print(f"âŒ æµ‹è¯•å¤±è´¥: {results['error']}")
                    sys.exit(1)
                
                print(f"âœ… ç»¼åˆæµ‹è¯•å®Œæˆ")
                summary = results['comprehensive_summary']
                print(f"   å»¶è¿Ÿ - å¹³å‡: {summary['latency_summary']['avg_ms']:.2f}ms, "
                      f"P95: {summary['latency_summary']['p95_ms']:.2f}ms")
                print(f"   QPS: {summary['qps_summary']['qps']:.2f}, "
                      f"é”™è¯¯ç‡: {summary['qps_summary']['error_rate']:.2%}")
                print(f"   å»¶è¿ŸæŠ¥å‘Š: {results['latency_report']}")
                print(f"   QPS æŠ¥å‘Š: {results['qps_report']}")
                print(f"   ç»¼åˆæ‘˜è¦: {results['summary_file']}")
                
            elif args.test_type == 'dataset':
                if not args.dataset:
                    print("âŒ è¯·æŒ‡å®šæµ‹è¯•é›†åç§° (--dataset)")
                    print("å¯ç”¨æµ‹è¯•é›†:")
                    for dataset in tester.list_available_datasets():
                        info = tester.get_dataset_info(dataset)
                        if info:
                            print(f"  - {dataset}: {info.get('name', 'N/A')} ({info.get('total_samples', 0)} æ ·æœ¬)")
                    sys.exit(1)
                
                print(f"ğŸ“Š å¼€å§‹æµ‹è¯•é›†è¯„ä¼°: {args.dataset}")
                try:
                    report = tester.run_dataset_evaluation(
                        args.model, args.dataset, 
                        args.sample_count, args.categories
                    )
                    print(f"âœ… æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
                    print(f"   æ€»æ ·æœ¬æ•°: {report.total_samples}")
                    print(f"   æˆåŠŸæµ‹è¯•æ•°: {report.successful_tests}")
                    print(f"   æˆåŠŸç‡: {report.successful_tests/report.total_samples:.2%}")
                    print(f"   è¯„åˆ†å‡†ç¡®æ€§: {report.avg_score_accuracy:.2%}")
                    print(f"   ç±»åˆ«å‡†ç¡®æ€§: {report.category_accuracy:.2%}")
                    print(f"   å¹³å‡å“åº”æ—¶é—´: {report.avg_response_time_ms:.2f}ms")
                except Exception as e:
                    print(f"âŒ æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}")
                    sys.exit(1)
                    
            elif args.test_type == 'all-datasets':
                print(f"ğŸ“Š å¼€å§‹æ‰€æœ‰æµ‹è¯•é›†è¯„ä¼°...")
                try:
                    reports = tester.run_all_dataset_evaluations(args.model, args.sample_count)
                    print(f"âœ… æ‰€æœ‰æµ‹è¯•é›†è¯„ä¼°å®Œæˆ")
                    print(f"   è¯„ä¼°äº† {len(reports)} ä¸ªæµ‹è¯•é›†")
                    
                    for dataset_name, report in reports.items():
                        print(f"   ğŸ“‹ {dataset_name}:")
                        print(f"     - æ ·æœ¬æ•°: {report.total_samples}")
                        print(f"     - æˆåŠŸç‡: {report.successful_tests/report.total_samples:.2%}")
                        print(f"     - è¯„åˆ†å‡†ç¡®æ€§: {report.avg_score_accuracy:.2%}")
                        print(f"     - ç±»åˆ«å‡†ç¡®æ€§: {report.category_accuracy:.2%}")
                        
                except Exception as e:
                    print(f"âŒ æ‰€æœ‰æµ‹è¯•é›†è¯„ä¼°å¤±è´¥: {e}")
                    sys.exit(1)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(0)
    except Exception as e:
        print(f"âŒ æµ‹è¯•æ‰§è¡Œé”™è¯¯: {e}")
        sys.exit(1)