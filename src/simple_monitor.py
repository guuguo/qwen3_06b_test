#!/usr/bin/env python3
"""
Qwen-3 ç®€åŒ–æ€§èƒ½ç›‘æ§æ¨¡å—

æä¾›è½»é‡çº§çš„æ€§èƒ½ç›‘æ§åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
- æ–‡ä»¶æ—¥å¿—ç›‘æ§å’Œç®¡ç†
- ç³»ç»ŸæŒ‡æ ‡æ”¶é›†ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ï¼‰
- æ—¥åº¦æ‘˜è¦æŠ¥å‘Šç”Ÿæˆ
- å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†
- æŒ‡æ ‡æ•°æ®æŒä¹…åŒ–å­˜å‚¨

ä½œè€…: Qwen-3 éƒ¨ç½²å›¢é˜Ÿ
ç‰ˆæœ¬: 1.0.0
"""

import os
import json
import time
import psutil
import threading
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from pathlib import Path
import logging
from collections import deque, defaultdict
import statistics


@dataclass
class SystemMetrics:
    """ç³»ç»ŸæŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    cpu_percent: float
    memory_percent: float
    memory_used_gb: float
    memory_available_gb: float
    disk_usage_percent: float
    disk_used_gb: float
    disk_free_gb: float
    load_average: Optional[Tuple[float, float, float]] = None
    network_io: Optional[Dict[str, int]] = None


@dataclass
class RequestMetrics:
    """è¯·æ±‚æŒ‡æ ‡æ•°æ®ç±»"""
    timestamp: str
    model: str
    prompt_length: int
    response_length: int
    latency_ms: float
    status: str
    error: Optional[str] = None
    tokens_per_second: Optional[float] = None
    memory_usage_mb: Optional[float] = None


@dataclass
class DailySummary:
    """æ—¥åº¦æ‘˜è¦æ•°æ®ç±»"""
    date: str
    total_requests: int
    successful_requests: int
    error_requests: int
    avg_latency_ms: float
    max_latency_ms: float
    min_latency_ms: float
    p95_latency_ms: float
    avg_cpu_percent: float
    max_memory_percent: float
    avg_memory_gb: float
    total_tokens: int
    avg_tokens_per_second: float


class SimpleFileMonitor:
    """
    ç®€å•æ–‡ä»¶æ—¥å¿—ç›‘æ§å™¨
    
    æä¾›åŸºäºæ–‡ä»¶çš„æ—¥å¿—è®°å½•å’Œç›‘æ§åŠŸèƒ½ï¼Œé€‚ç”¨äºæœ¬åœ°å¼€å‘ç¯å¢ƒã€‚
    """
    
    def __init__(self, log_dir: str = "./logs", max_log_files: int = 30):
        """
        åˆå§‹åŒ–æ–‡ä»¶ç›‘æ§å™¨
        
        Args:
            log_dir: æ—¥å¿—ç›®å½•è·¯å¾„
            max_log_files: æœ€å¤§ä¿ç•™æ—¥å¿—æ–‡ä»¶æ•°
        """
        self.log_dir = Path(log_dir)
        self.max_log_files = max_log_files
        
        # åˆ›å»ºæ—¥å¿—ç›®å½•
        self.log_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # å†…å­˜ç¼“å­˜
        self._request_cache = deque(maxlen=1000)  # æœ€è¿‘1000æ¡è¯·æ±‚
        self._system_cache = deque(maxlen=288)    # æœ€è¿‘24å°æ—¶ç³»ç»ŸæŒ‡æ ‡ï¼ˆ5åˆ†é’Ÿé—´éš”ï¼‰
        
        self.logger.info(f"æ–‡ä»¶ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ: {self.log_dir}")
    
    def log_request(self, 
                   model: str, 
                   prompt: str, 
                   response: str, 
                   latency_ms: float, 
                   error: str = None,
                   tokens_per_second: float = None) -> None:
        """
        è®°å½•è¯·æ±‚æ—¥å¿—
        
        Args:
            model: æ¨¡å‹åç§°
            prompt: è¾“å…¥æç¤º
            response: æ¨¡å‹å“åº”
            latency_ms: å»¶è¿Ÿï¼ˆæ¯«ç§’ï¼‰
            error: é”™è¯¯ä¿¡æ¯
            tokens_per_second: æ¯ç§’å¤„ç†çš„tokenæ•°
        """
        timestamp = datetime.now().isoformat()
        
        # åˆ›å»ºè¯·æ±‚æŒ‡æ ‡
        metrics = RequestMetrics(
            timestamp=timestamp,
            model=model,
            prompt_length=len(prompt),
            response_length=len(response) if response else 0,
            latency_ms=latency_ms,
            status="error" if error else "success",
            error=error,
            tokens_per_second=tokens_per_second
        )
        
        # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
        self._request_cache.append(metrics)
        
        # å†™å…¥æ—¥å¿—æ–‡ä»¶
        log_file = self.log_dir / f"requests_{datetime.now().strftime('%Y%m%d')}.jsonl"
        try:
            with open(log_file, "a", encoding="utf-8") as f:
                f.write(json.dumps(asdict(metrics), ensure_ascii=False) + "\n")
        except Exception as e:
            self.logger.error(f"å†™å…¥è¯·æ±‚æ—¥å¿—å¤±è´¥: {e}")
    
    def log_system_metrics(self) -> SystemMetrics:
        """
        è®°å½•ç³»ç»ŸæŒ‡æ ‡
        
        Returns:
            SystemMetrics: å½“å‰ç³»ç»ŸæŒ‡æ ‡
        """
        try:
            # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            cpu_percent = psutil.cpu_percent(interval=1)
            memory = psutil.virtual_memory()
            disk = psutil.disk_usage('/')
            
            # è·å–è´Ÿè½½å¹³å‡å€¼ï¼ˆä»…Linux/macOSï¼‰
            load_avg = None
            try:
                if hasattr(os, 'getloadavg'):
                    load_avg = os.getloadavg()
            except (OSError, AttributeError):
                pass
            
            # è·å–ç½‘ç»œIOï¼ˆå¯é€‰ï¼‰
            network_io = None
            try:
                net_io = psutil.net_io_counters()
                if net_io:
                    network_io = {
                        'bytes_sent': net_io.bytes_sent,
                        'bytes_recv': net_io.bytes_recv,
                        'packets_sent': net_io.packets_sent,
                        'packets_recv': net_io.packets_recv
                    }
            except Exception:
                pass
            
            # åˆ›å»ºç³»ç»ŸæŒ‡æ ‡
            metrics = SystemMetrics(
                timestamp=datetime.now().isoformat(),
                cpu_percent=cpu_percent,
                memory_percent=memory.percent,
                memory_used_gb=memory.used / (1024**3),
                memory_available_gb=memory.available / (1024**3),
                disk_usage_percent=disk.percent,
                disk_used_gb=disk.used / (1024**3),
                disk_free_gb=disk.free / (1024**3),
                load_average=load_avg,
                network_io=network_io
            )
            
            # æ·»åŠ åˆ°å†…å­˜ç¼“å­˜
            self._system_cache.append(metrics)
            
            # å†™å…¥æ—¥å¿—æ–‡ä»¶
            log_file = self.log_dir / f"system_{datetime.now().strftime('%Y%m%d')}.jsonl"
            try:
                with open(log_file, "a", encoding="utf-8") as f:
                    f.write(json.dumps(asdict(metrics), ensure_ascii=False) + "\n")
            except Exception as e:
                self.logger.error(f"å†™å…¥ç³»ç»Ÿæ—¥å¿—å¤±è´¥: {e}")
            
            return metrics
            
        except Exception as e:
            self.logger.error(f"æ”¶é›†ç³»ç»ŸæŒ‡æ ‡å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤å€¼
            return SystemMetrics(
                timestamp=datetime.now().isoformat(),
                cpu_percent=0.0,
                memory_percent=0.0,
                memory_used_gb=0.0,
                memory_available_gb=0.0,
                disk_usage_percent=0.0,
                disk_used_gb=0.0,
                disk_free_gb=0.0
            )
    
    def get_recent_requests(self, limit: int = 100) -> List[RequestMetrics]:
        """
        è·å–æœ€è¿‘çš„è¯·æ±‚è®°å½•
        
        Args:
            limit: è¿”å›è®°å½•æ•°é™åˆ¶
            
        Returns:
            List[RequestMetrics]: æœ€è¿‘çš„è¯·æ±‚è®°å½•
        """
        return list(self._request_cache)[-limit:]
    
    def get_recent_system_metrics(self, limit: int = 50) -> List[SystemMetrics]:
        """
        è·å–æœ€è¿‘çš„ç³»ç»ŸæŒ‡æ ‡
        
        Args:
            limit: è¿”å›è®°å½•æ•°é™åˆ¶
            
        Returns:
            List[SystemMetrics]: æœ€è¿‘çš„ç³»ç»ŸæŒ‡æ ‡
        """
        return list(self._system_cache)[-limit:]
    
    def generate_daily_summary(self, date: str = None) -> DailySummary:
        """
        ç”Ÿæˆæ—¥åº¦æ‘˜è¦æŠ¥å‘Š
        
        Args:
            date: æ—¥æœŸå­—ç¬¦ä¸² (YYYYMMDD)ï¼ŒNoneè¡¨ç¤ºä»Šå¤©
            
        Returns:
            DailySummary: æ—¥åº¦æ‘˜è¦
        """
        if not date:
            date = datetime.now().strftime('%Y%m%d')
        
        # è¯»å–è¯·æ±‚æ—¥å¿—
        request_file = self.log_dir / f"requests_{date}.jsonl"
        system_file = self.log_dir / f"system_{date}.jsonl"
        
        # åˆå§‹åŒ–æ‘˜è¦
        summary = DailySummary(
            date=date,
            total_requests=0,
            successful_requests=0,
            error_requests=0,
            avg_latency_ms=0.0,
            max_latency_ms=0.0,
            min_latency_ms=0.0,
            p95_latency_ms=0.0,
            avg_cpu_percent=0.0,
            max_memory_percent=0.0,
            avg_memory_gb=0.0,
            total_tokens=0,
            avg_tokens_per_second=0.0
        )
        
        # åˆ†æè¯·æ±‚æ—¥å¿—
        if request_file.exists():
            latencies = []
            tokens_per_second_list = []
            
            try:
                with open(request_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            entry = json.loads(line.strip())
                            summary.total_requests += 1
                            
                            if entry.get("status") == "success":
                                summary.successful_requests += 1
                                latencies.append(entry["latency_ms"])
                                
                                # ç»Ÿè®¡tokenæ•°
                                response_length = entry.get("response_length", 0)
                                summary.total_tokens += response_length
                                
                                # ç»Ÿè®¡TPS
                                tps = entry.get("tokens_per_second")
                                if tps:
                                    tokens_per_second_list.append(tps)
                            else:
                                summary.error_requests += 1
                                
                        except (json.JSONDecodeError, KeyError) as e:
                            self.logger.warning(f"è§£æè¯·æ±‚æ—¥å¿—è¡Œå¤±è´¥: {e}")
                            continue
                            
            except Exception as e:
                self.logger.error(f"è¯»å–è¯·æ±‚æ—¥å¿—å¤±è´¥: {e}")
            
            # è®¡ç®—å»¶è¿Ÿç»Ÿè®¡
            if latencies:
                summary.avg_latency_ms = statistics.mean(latencies)
                summary.max_latency_ms = max(latencies)
                summary.min_latency_ms = min(latencies)
                summary.p95_latency_ms = statistics.quantiles(latencies, n=20)[18]  # 95th percentile
            
            # è®¡ç®—TPSç»Ÿè®¡
            if tokens_per_second_list:
                summary.avg_tokens_per_second = statistics.mean(tokens_per_second_list)
        
        # åˆ†æç³»ç»Ÿæ—¥å¿—
        if system_file.exists():
            cpu_values = []
            memory_values = []
            memory_gb_values = []
            
            try:
                with open(system_file, 'r', encoding='utf-8') as f:
                    for line in f:
                        try:
                            entry = json.loads(line.strip())
                            cpu_values.append(entry["cpu_percent"])
                            memory_values.append(entry["memory_percent"])
                            memory_gb_values.append(entry["memory_used_gb"])
                        except (json.JSONDecodeError, KeyError) as e:
                            self.logger.warning(f"è§£æç³»ç»Ÿæ—¥å¿—è¡Œå¤±è´¥: {e}")
                            continue
                            
            except Exception as e:
                self.logger.error(f"è¯»å–ç³»ç»Ÿæ—¥å¿—å¤±è´¥: {e}")
            
            # è®¡ç®—ç³»ç»ŸæŒ‡æ ‡ç»Ÿè®¡
            if cpu_values:
                summary.avg_cpu_percent = statistics.mean(cpu_values)
            if memory_values:
                summary.max_memory_percent = max(memory_values)
            if memory_gb_values:
                summary.avg_memory_gb = statistics.mean(memory_gb_values)
        
        return summary
    
    def cleanup_old_logs(self) -> int:
        """
        æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
        
        Returns:
            int: åˆ é™¤çš„æ–‡ä»¶æ•°é‡
        """
        deleted_count = 0
        
        try:
            # è·å–æ‰€æœ‰æ—¥å¿—æ–‡ä»¶
            log_files = []
            for pattern in ["requests_*.jsonl", "system_*.jsonl"]:
                log_files.extend(self.log_dir.glob(pattern))
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
            log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            # åˆ é™¤è¶…è¿‡é™åˆ¶çš„æ–‡ä»¶
            if len(log_files) > self.max_log_files:
                for log_file in log_files[self.max_log_files:]:
                    try:
                        log_file.unlink()
                        deleted_count += 1
                        self.logger.info(f"åˆ é™¤æ—§æ—¥å¿—æ–‡ä»¶: {log_file.name}")
                    except Exception as e:
                        self.logger.error(f"åˆ é™¤æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")
                        
        except Exception as e:
            self.logger.error(f"æ¸…ç†æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        
        return deleted_count
    
    def get_log_files_info(self) -> Dict[str, Any]:
        """
        è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
        
        Returns:
            Dict[str, Any]: æ—¥å¿—æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯
        """
        info = {
            'total_files': 0,
            'total_size_mb': 0.0,
            'oldest_file': None,
            'newest_file': None,
            'files_by_type': defaultdict(int)
        }
        
        try:
            log_files = list(self.log_dir.glob("*.jsonl"))
            info['total_files'] = len(log_files)
            
            if log_files:
                # è®¡ç®—æ€»å¤§å°
                total_size = sum(f.stat().st_size for f in log_files)
                info['total_size_mb'] = total_size / (1024 * 1024)
                
                # æ‰¾åˆ°æœ€æ–°å’Œæœ€æ—§çš„æ–‡ä»¶
                log_files.sort(key=lambda x: x.stat().st_mtime)
                info['oldest_file'] = log_files[0].name
                info['newest_file'] = log_files[-1].name
                
                # æŒ‰ç±»å‹ç»Ÿè®¡
                for log_file in log_files:
                    if log_file.name.startswith('requests_'):
                        info['files_by_type']['requests'] += 1
                    elif log_file.name.startswith('system_'):
                        info['files_by_type']['system'] += 1
                    else:
                        info['files_by_type']['other'] += 1
                        
        except Exception as e:
            self.logger.error(f"è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {e}")
        
        return dict(info)


class SimplePerformanceMonitor:
    """
    ç®€å•æ€§èƒ½ç›‘æ§å™¨
    
    æä¾›å®æ—¶æ€§èƒ½æŒ‡æ ‡æ”¶é›†å’Œåˆ†æåŠŸèƒ½ã€‚
    """
    
    def __init__(self, 
                 file_monitor: SimpleFileMonitor,
                 collection_interval: int = 60):
        """
        åˆå§‹åŒ–æ€§èƒ½ç›‘æ§å™¨
        
        Args:
            file_monitor: æ–‡ä»¶ç›‘æ§å™¨å®ä¾‹
            collection_interval: æŒ‡æ ‡æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
        """
        self.file_monitor = file_monitor
        self.collection_interval = collection_interval
        
        # è®¾ç½®æ—¥å¿—
        self.logger = logging.getLogger(__name__)
        
        # ç›‘æ§çŠ¶æ€
        self._monitoring = False
        self._monitor_thread = None
        
        # å®æ—¶æŒ‡æ ‡ç¼“å­˜
        self._current_metrics = {}
        self._metrics_lock = threading.Lock()
        
        self.logger.info("æ€§èƒ½ç›‘æ§å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def start_monitoring(self) -> None:
        """å¼€å§‹ç›‘æ§"""
        if self._monitoring:
            self.logger.warning("ç›‘æ§å·²åœ¨è¿è¡Œä¸­")
            return
        
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            daemon=True,
            name="PerformanceMonitor"
        )
        self._monitor_thread.start()
        
        self.logger.info(f"æ€§èƒ½ç›‘æ§å·²å¯åŠ¨ï¼Œæ”¶é›†é—´éš”: {self.collection_interval}ç§’")
    
    def stop_monitoring(self) -> None:
        """åœæ­¢ç›‘æ§"""
        if not self._monitoring:
            return
        
        self._monitoring = False
        if self._monitor_thread and self._monitor_thread.is_alive():
            self._monitor_thread.join(timeout=5)
        
        self.logger.info("æ€§èƒ½ç›‘æ§å·²åœæ­¢")
    
    def _monitoring_loop(self) -> None:
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                metrics = self.file_monitor.log_system_metrics()
                
                # æ›´æ–°å½“å‰æŒ‡æ ‡
                with self._metrics_lock:
                    self._current_metrics = asdict(metrics)
                
                # ç­‰å¾…ä¸‹æ¬¡æ”¶é›†
                time.sleep(self.collection_interval)
                
            except Exception as e:
                self.logger.error(f"ç›‘æ§å¾ªç¯å¼‚å¸¸: {e}")
                time.sleep(5)  # å‡ºé”™æ—¶çŸ­æš‚ç­‰å¾…
    
    def get_current_metrics(self) -> Dict[str, Any]:
        """
        è·å–å½“å‰æŒ‡æ ‡
        
        Returns:
            Dict[str, Any]: å½“å‰ç³»ç»ŸæŒ‡æ ‡
        """
        with self._metrics_lock:
            return self._current_metrics.copy()
    
    def get_performance_summary(self, hours: int = 24) -> Dict[str, Any]:
        """
        è·å–æ€§èƒ½æ‘˜è¦
        
        Args:
            hours: ç»Ÿè®¡æ—¶é—´èŒƒå›´ï¼ˆå°æ—¶ï¼‰
            
        Returns:
            Dict[str, Any]: æ€§èƒ½æ‘˜è¦
        """
        # è·å–æœ€è¿‘çš„ç³»ç»ŸæŒ‡æ ‡
        recent_metrics = self.file_monitor.get_recent_system_metrics(
            limit=hours * 12  # å‡è®¾5åˆ†é’Ÿé—´éš”
        )
        
        # è·å–æœ€è¿‘çš„è¯·æ±‚è®°å½•
        recent_requests = self.file_monitor.get_recent_requests(limit=1000)
        
        summary = {
            'time_range_hours': hours,
            'system_metrics': {
                'avg_cpu_percent': 0.0,
                'max_cpu_percent': 0.0,
                'avg_memory_percent': 0.0,
                'max_memory_percent': 0.0,
                'avg_memory_gb': 0.0,
                'disk_usage_percent': 0.0
            },
            'request_metrics': {
                'total_requests': len(recent_requests),
                'successful_requests': 0,
                'error_requests': 0,
                'avg_latency_ms': 0.0,
                'max_latency_ms': 0.0,
                'requests_per_hour': 0.0
            }
        }
        
        # åˆ†æç³»ç»ŸæŒ‡æ ‡
        if recent_metrics:
            cpu_values = [m.cpu_percent for m in recent_metrics]
            memory_values = [m.memory_percent for m in recent_metrics]
            memory_gb_values = [m.memory_used_gb for m in recent_metrics]
            
            summary['system_metrics'].update({
                'avg_cpu_percent': statistics.mean(cpu_values),
                'max_cpu_percent': max(cpu_values),
                'avg_memory_percent': statistics.mean(memory_values),
                'max_memory_percent': max(memory_values),
                'avg_memory_gb': statistics.mean(memory_gb_values),
                'disk_usage_percent': recent_metrics[-1].disk_usage_percent
            })
        
        # åˆ†æè¯·æ±‚æŒ‡æ ‡
        if recent_requests:
            successful = [r for r in recent_requests if r.status == "success"]
            error = [r for r in recent_requests if r.status != "success"]
            
            summary['request_metrics'].update({
                'successful_requests': len(successful),
                'error_requests': len(error),
                'requests_per_hour': len(recent_requests) / hours
            })
            
            if successful:
                latencies = [r.latency_ms for r in successful]
                summary['request_metrics'].update({
                    'avg_latency_ms': statistics.mean(latencies),
                    'max_latency_ms': max(latencies)
                })
        
        return summary
    
    def is_monitoring(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æ­£åœ¨ç›‘æ§"""
        return self._monitoring
    
    def __enter__(self):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        self.start_monitoring()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """ä¸Šä¸‹æ–‡ç®¡ç†å™¨å‡ºå£"""
        self.stop_monitoring()


# ä¾¿æ·å‡½æ•°
def create_simple_monitor(log_dir: str = "./logs", 
                         collection_interval: int = 60) -> Tuple[SimpleFileMonitor, SimplePerformanceMonitor]:
    """
    åˆ›å»ºç®€å•ç›‘æ§å™¨çš„ä¾¿æ·å‡½æ•°
    
    Args:
        log_dir: æ—¥å¿—ç›®å½•
        collection_interval: æ”¶é›†é—´éš”ï¼ˆç§’ï¼‰
        
    Returns:
        Tuple[SimpleFileMonitor, SimplePerformanceMonitor]: æ–‡ä»¶ç›‘æ§å™¨å’Œæ€§èƒ½ç›‘æ§å™¨
    """
    file_monitor = SimpleFileMonitor(log_dir)
    perf_monitor = SimplePerformanceMonitor(file_monitor, collection_interval)
    
    return file_monitor, perf_monitor


if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    import sys
    import signal
    
    # è®¾ç½®æ—¥å¿—
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    print("ğŸš€ å¯åŠ¨ç®€å•ç›‘æ§å™¨æµ‹è¯•...")
    
    # åˆ›å»ºç›‘æ§å™¨
    file_monitor, perf_monitor = create_simple_monitor()
    
    def signal_handler(signum, frame):
        print("\nğŸ›‘ æ”¶åˆ°åœæ­¢ä¿¡å·ï¼Œæ­£åœ¨å…³é—­ç›‘æ§å™¨...")
        perf_monitor.stop_monitoring()
        sys.exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    try:
        # å¯åŠ¨æ€§èƒ½ç›‘æ§
        perf_monitor.start_monitoring()
        
        # æ¨¡æ‹Ÿä¸€äº›è¯·æ±‚æ—¥å¿—
        print("ğŸ“ è®°å½•æµ‹è¯•è¯·æ±‚...")
        file_monitor.log_request(
            model="qwen3:0.6b",
            prompt="æµ‹è¯•æç¤º",
            response="æµ‹è¯•å“åº”å†…å®¹",
            latency_ms=1234.5,
            tokens_per_second=15.2
        )
        
        # ç­‰å¾…ä¸€äº›æŒ‡æ ‡æ”¶é›†
        print("â³ ç­‰å¾…æŒ‡æ ‡æ”¶é›†...")
        time.sleep(5)
        
        # æ˜¾ç¤ºå½“å‰æŒ‡æ ‡
        current = perf_monitor.get_current_metrics()
        if current:
            print(f"ğŸ“Š å½“å‰ç³»ç»ŸæŒ‡æ ‡:")
            print(f"   CPU: {current.get('cpu_percent', 0):.1f}%")
            print(f"   å†…å­˜: {current.get('memory_percent', 0):.1f}%")
            print(f"   ç£ç›˜: {current.get('disk_usage_percent', 0):.1f}%")
        
        # ç”Ÿæˆæ‘˜è¦æŠ¥å‘Š
        print("ğŸ“‹ ç”Ÿæˆæ—¥åº¦æ‘˜è¦...")
        summary = file_monitor.generate_daily_summary()
        print(f"   æ€»è¯·æ±‚æ•°: {summary.total_requests}")
        print(f"   æˆåŠŸè¯·æ±‚: {summary.successful_requests}")
        print(f"   å¹³å‡å»¶è¿Ÿ: {summary.avg_latency_ms:.2f}ms")
        
        # è·å–æ—¥å¿—æ–‡ä»¶ä¿¡æ¯
        log_info = file_monitor.get_log_files_info()
        print(f"ğŸ“ æ—¥å¿—æ–‡ä»¶ä¿¡æ¯:")
        print(f"   æ–‡ä»¶æ•°é‡: {log_info['total_files']}")
        print(f"   æ€»å¤§å°: {log_info['total_size_mb']:.2f}MB")
        
        print("âœ… ç›‘æ§å™¨æµ‹è¯•å®Œæˆï¼æŒ‰ Ctrl+C åœæ­¢ç›‘æ§...")
        
        # ä¿æŒè¿è¡Œ
        while True:
            time.sleep(1)
            
    except KeyboardInterrupt:
        print("\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ï¼Œæ­£åœ¨åœæ­¢...")
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¼‚å¸¸: {e}")
    finally:
        perf_monitor.stop_monitoring()
        print("ğŸ‘‹ ç›‘æ§å™¨å·²åœæ­¢")
