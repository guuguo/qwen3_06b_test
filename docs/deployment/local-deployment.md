# Qwen-3 æœ¬åœ°éƒ¨ç½²æŒ‡å—

æœ¬æŒ‡å—å°†å¸®åŠ©ä½ åœ¨æœ¬åœ°ç¯å¢ƒéƒ¨ç½² Qwen-3 æ¨¡å‹ï¼ŒåŒ…æ‹¬ Ollama é›†æˆå’Œç®€åŒ–ç›‘æ§æ–¹æ¡ˆã€‚

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

### ç³»ç»Ÿè¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: macOS 10.15+ æˆ– Linux (Ubuntu 18.04+)
- **å†…å­˜**: 8GB+ (æ¨è 16GB)
- **å­˜å‚¨**: 20GB+ å¯ç”¨ç©ºé—´
- **Python**: 3.8+

### ç¡¬ä»¶è¦æ±‚
- **CPU**: 4æ ¸å¿ƒä»¥ä¸Š (æ¨è 8æ ¸å¿ƒ)
- **å†…å­˜**: 8GB+ (0.6B æ¨¡å‹éœ€è¦çº¦ 2-4GBï¼Œ1.7B æ¨¡å‹éœ€è¦çº¦ 4-8GB)
- **å­˜å‚¨**: SSD æ¨èï¼Œç”¨äºæ›´å¿«çš„æ¨¡å‹åŠ è½½

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤ 1: å®‰è£… Ollama

å¦‚æœä½ è¿˜æ²¡æœ‰å®‰è£… Ollamaï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤ï¼š

```bash
# macOS
curl -fsSL https://ollama.ai/install.sh | sh

# æˆ–è€…ä½¿ç”¨ Homebrew
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh
```

### æ­¥éª¤ 2: å¯åŠ¨ Ollama æœåŠ¡

```bash
# å¯åŠ¨ Ollama æœåŠ¡
ollama serve

# éªŒè¯æœåŠ¡çŠ¶æ€
curl http://localhost:11434/api/tags
```

### æ­¥éª¤ 3: ä¸‹è½½ Qwen-3 æ¨¡å‹

```bash
# ä¸‹è½½ Qwen-3 0.6B æ¨¡å‹
ollama pull qwen3:0.6b

# ä¸‹è½½ Qwen-3 1.7B æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
ollama pull qwen3:1.7b

# éªŒè¯æ¨¡å‹ä¸‹è½½
ollama list
```

### æ­¥éª¤ 4: æµ‹è¯•æ¨¡å‹

```bash
# æµ‹è¯• 0.6B æ¨¡å‹
ollama run qwen3:0.6b "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"

# æµ‹è¯• 1.7B æ¨¡å‹
ollama run qwen3:1.7b "Hello, can you introduce yourself?"
```

## ğŸ”§ Python ç¯å¢ƒè®¾ç½®

### åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir qwen3-deployment
cd qwen3-deployment

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# macOS/Linux
source venv/bin/activate

# Windows
# venv\Scripts\activate
```

### å®‰è£…ä¾èµ–

åˆ›å»º `requirements.txt` æ–‡ä»¶ï¼š

```txt
requests>=2.28.0
psutil>=5.9.0
flask>=2.3.0
transformers>=4.30.0
torch>=2.0.0
numpy>=1.24.0
pandas>=2.0.0
matplotlib>=3.7.0
seaborn>=0.12.0
tqdm>=4.65.0
pyyaml>=6.0
click>=8.1.0
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt
```

## ğŸ“Š éƒ¨ç½²ç›‘æ§ç»„ä»¶

### åˆ›å»ºé¡¹ç›®ç»“æ„

```bash
mkdir -p {src,logs,config,tests,scripts}
touch src/__init__.py
```

### åŸºç¡€é…ç½®æ–‡ä»¶

åˆ›å»º `config/local_config.yaml`ï¼š

```yaml
# Qwen-3 æœ¬åœ°éƒ¨ç½²é…ç½®
ollama:
  base_url: "http://localhost:11434"
  timeout: 30
  models:
    - "qwen3:0.6b"
    - "qwen3:1.7b"

monitoring:
  log_dir: "./logs"
  metrics_interval: 60  # ç§’
  web_dashboard:
    host: "0.0.0.0"
    port: 5000
    debug: true

performance_testing:
  default_model: "qwen3:0.6b"
  test_prompts:
    - "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
    - "è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µã€‚"
    - "å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—é˜¶ä¹˜ã€‚"
    - "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ"
    - "è¯·æè¿°ä¸€ä¸‹æ·±åº¦å­¦ä¹ çš„å·¥ä½œåŸç†ã€‚"
  
  qps_test:
    concurrent_users: 5
    duration: 60
    
  latency_test:
    iterations: 100
```

### æ ¸å¿ƒä»£ç æ–‡ä»¶

åˆ›å»º `src/ollama_integration.py`ï¼š

```python
import requests
import json
import time
from datetime import datetime
from typing import Dict, List, Optional
import logging

class OllamaIntegration:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.session = requests.Session()
        self.logger = logging.getLogger(__name__)
        
    def check_ollama_status(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags", timeout=5)
            return response.status_code == 200
        except Exception as e:
            self.logger.error(f"Ollama æœåŠ¡æ£€æŸ¥å¤±è´¥: {e}")
            return False
            
    def list_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags")
            if response.status_code == 200:
                data = response.json()
                return [model['name'] for model in data.get('models', [])]
            return []
        except Exception as e:
            self.logger.error(f"è·å–æ¨¡å‹åˆ—è¡¨å¤±è´¥: {e}")
            return []
            
    def inference_with_metrics(self, model: str, prompt: str, **kwargs) -> Dict:
        """å¸¦æŒ‡æ ‡æ”¶é›†çš„æ¨ç†"""
        start_time = time.time()
        
        request_data = {
            "model": model,
            "prompt": prompt,
            "stream": False,
            **kwargs
        }
        
        try:
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json=request_data,
                timeout=kwargs.get('timeout', 30)
            )
            end_time = time.time()
            
            if response.status_code == 200:
                result_data = response.json()
                result = {
                    "response": result_data.get("response", ""),
                    "model": model,
                    "prompt_length": len(prompt),
                    "response_length": len(result_data.get("response", "")),
                    "latency_ms": (end_time - start_time) * 1000,
                    "status_code": response.status_code,
                    "timestamp": datetime.now().isoformat(),
                    "status": "success"
                }
            else:
                result = {
                    "error": f"HTTP {response.status_code}: {response.text}",
                    "model": model,
                    "prompt_length": len(prompt),
                    "latency_ms": (end_time - start_time) * 1000,
                    "status_code": response.status_code,
                    "timestamp": datetime.now().isoformat(),
                    "status": "error"
                }
                
        except Exception as e:
            end_time = time.time()
            result = {
                "error": str(e),
                "model": model,
                "prompt_length": len(prompt),
                "latency_ms": (end_time - start_time) * 1000,
                "timestamp": datetime.now().isoformat(),
                "status": "error"
            }
            
        return result

    def health_check(self) -> Dict:
        """å¥åº·æ£€æŸ¥"""
        try:
            start_time = time.time()
            response = self.session.get(f"{self.base_url}/api/tags", timeout=5)
            latency = (time.time() - start_time) * 1000
            
            if response.status_code == 200:
                models = self.list_models()
                return {
                    "status": "healthy",
                    "latency_ms": latency,
                    "models_count": len(models),
                    "models": models,
                    "timestamp": datetime.now().isoformat()
                }
            else:
                return {
                    "status": "unhealthy",
                    "error": f"HTTP {response.status_code}",
                    "latency_ms": latency,
                    "timestamp": datetime.now().isoformat()
                }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
```

### å¯åŠ¨è„šæœ¬

åˆ›å»º `scripts/start_local_monitoring.py`ï¼š

```python
#!/usr/bin/env python3
"""
Qwen-3 æœ¬åœ°ç›‘æ§å¯åŠ¨è„šæœ¬
"""

import os
import sys
import yaml
import logging
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.ollama_integration import OllamaIntegration

def setup_logging(log_level="INFO"):
    """è®¾ç½®æ—¥å¿—"""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('logs/app.log')
        ]
    )

def load_config(config_path="config/local_config.yaml"):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    except Exception as e:
        print(f"âŒ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥: {e}")
        return None

def check_prerequisites():
    """æ£€æŸ¥å…ˆå†³æ¡ä»¶"""
    print("ğŸ” æ£€æŸ¥ç¯å¢ƒå…ˆå†³æ¡ä»¶...")
    
    # æ£€æŸ¥ Python ç‰ˆæœ¬
    if sys.version_info < (3, 8):
        print("âŒ Python ç‰ˆæœ¬éœ€è¦ 3.8+")
        return False
        
    # æ£€æŸ¥å¿…è¦ç›®å½•
    for dir_name in ['logs', 'config']:
        os.makedirs(dir_name, exist_ok=True)
        
    print("âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡")
    return True

def main():
    parser = argparse.ArgumentParser(description="Qwen-3 æœ¬åœ°ç›‘æ§")
    parser.add_argument("--config", default="config/local_config.yaml", help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--log-level", default="INFO", help="æ—¥å¿—çº§åˆ«")
    parser.add_argument("--test", action="store_true", help="è¿è¡Œå¿«é€Ÿæµ‹è¯•")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥å…ˆå†³æ¡ä»¶
    if not check_prerequisites():
        sys.exit(1)
        
    # è®¾ç½®æ—¥å¿—
    setup_logging(args.log_level)
    logger = logging.getLogger(__name__)
    
    # åŠ è½½é…ç½®
    config = load_config(args.config)
    if not config:
        sys.exit(1)
        
    # åˆå§‹åŒ– Ollama é›†æˆ
    ollama = OllamaIntegration(config['ollama']['base_url'])
    
    print("ğŸš€ å¯åŠ¨ Qwen-3 æœ¬åœ°ç›‘æ§...")
    
    # æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€
    if not ollama.check_ollama_status():
        print("âŒ Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ Ollama:")
        print("   ollama serve")
        sys.exit(1)
        
    print("âœ… Ollama æœåŠ¡æ­£å¸¸")
    
    # æ£€æŸ¥å¯ç”¨æ¨¡å‹
    models = ollama.list_models()
    if not models:
        print("âŒ æœªæ‰¾åˆ°å¯ç”¨æ¨¡å‹ï¼Œè¯·å…ˆä¸‹è½½æ¨¡å‹:")
        print("   ollama pull qwen3:0.6b")
        sys.exit(1)
        
    print(f"âœ… å‘ç°æ¨¡å‹: {', '.join(models)}")
    
    if args.test:
        # è¿è¡Œå¿«é€Ÿæµ‹è¯•
        print("ğŸ“Š è¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        test_prompt = "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
        result = ollama.inference_with_metrics("qwen3:0.6b", test_prompt)
        
        if result['status'] == 'success':
            print(f"âœ… æµ‹è¯•æˆåŠŸ!")
            print(f"   å»¶è¿Ÿ: {result['latency_ms']:.2f}ms")
            print(f"   å“åº”é•¿åº¦: {result['response_length']} å­—ç¬¦")
            print(f"   å“åº”å†…å®¹: {result['response'][:100]}...")
        else:
            print(f"âŒ æµ‹è¯•å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
    else:
        print("âœ… æœ¬åœ°ç›‘æ§å·²å°±ç»ª")
        print("ğŸ’¡ ä½¿ç”¨ --test å‚æ•°è¿è¡Œå¿«é€Ÿæµ‹è¯•")
        print("ğŸ’¡ ä½¿ç”¨ python scripts/performance_test.py è¿è¡Œæ€§èƒ½æµ‹è¯•")

if __name__ == "__main__":
    main()
```

## ğŸ§ª éªŒè¯éƒ¨ç½²

### è¿è¡Œå¥åº·æ£€æŸ¥

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd qwen3-deployment

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
source venv/bin/activate

# è¿è¡Œå¥åº·æ£€æŸ¥
python scripts/start_local_monitoring.py --test
```

æœŸæœ›è¾“å‡ºï¼š
```
ğŸ” æ£€æŸ¥ç¯å¢ƒå…ˆå†³æ¡ä»¶...
âœ… ç¯å¢ƒæ£€æŸ¥é€šè¿‡
ğŸš€ å¯åŠ¨ Qwen-3 æœ¬åœ°ç›‘æ§...
âœ… Ollama æœåŠ¡æ­£å¸¸
âœ… å‘ç°æ¨¡å‹: qwen3:0.6b
ğŸ“Š è¿è¡Œå¿«é€Ÿæµ‹è¯•...
âœ… æµ‹è¯•æˆåŠŸ!
   å»¶è¿Ÿ: 1234.56ms
   å“åº”é•¿åº¦: 89 å­—ç¬¦
   å“åº”å†…å®¹: ä½ å¥½ï¼æˆ‘æ˜¯Qwenï¼Œä¸€ä¸ªç”±é˜¿é‡Œäº‘å¼€å‘çš„å¤§å‹è¯­è¨€æ¨¡å‹...
```

## ğŸ”§ å¸¸è§é—®é¢˜æ’æŸ¥

### Ollama æœåŠ¡æ— æ³•å¯åŠ¨

```bash
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434

# é‡å¯ Ollama æœåŠ¡
pkill ollama
ollama serve
```

### æ¨¡å‹ä¸‹è½½å¤±è´¥

```bash
# æ£€æŸ¥ç½‘ç»œè¿æ¥
curl -I https://ollama.ai

# æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
ollama pull qwen3:0.6b --verbose
```

### Python ä¾èµ–å®‰è£…å¤±è´¥

```bash
# å‡çº§ pip
pip install --upgrade pip

# æ¸…é™¤ç¼“å­˜é‡æ–°å®‰è£…
pip cache purge
pip install -r requirements.txt --no-cache-dir
```

### å†…å­˜ä¸è¶³

- ç¡®ä¿è‡³å°‘æœ‰ 8GB å¯ç”¨å†…å­˜
- å…³é—­å…¶ä»–å ç”¨å†…å­˜çš„åº”ç”¨ç¨‹åº
- è€ƒè™‘ä½¿ç”¨ 0.6B æ¨¡å‹è€Œä¸æ˜¯ 1.7B æ¨¡å‹

## ğŸ“ˆ ä¸‹ä¸€æ­¥

éƒ¨ç½²æˆåŠŸåï¼Œä½ å¯ä»¥ï¼š

1. **è¿è¡Œæ€§èƒ½æµ‹è¯•**: å‚è€ƒ [æ€§èƒ½æµ‹è¯•æŒ‡å—](../performance/testing-guide.md)
2. **è®¾ç½®ç›‘æ§**: å‚è€ƒ [æœ¬åœ°ç›‘æ§æ–¹æ¡ˆ](../monitoring/local-monitoring.md)
3. **å¼€å§‹å¾®è°ƒ**: å‚è€ƒ [å¾®è°ƒæŒ‡å—](../fine-tuning/training-guide.md)

## ğŸ†˜ è·å–å¸®åŠ©

å¦‚æœé‡åˆ°é—®é¢˜ï¼š

1. æŸ¥çœ‹ [æ•…éšœæ’æŸ¥æŒ‡å—](../monitoring/troubleshooting.md)
2. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶ `logs/app.log`
3. ç¡®è®¤ç¯å¢ƒè¦æ±‚æ˜¯å¦æ»¡è¶³
4. æäº¤ Issue å¹¶é™„ä¸Šè¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
