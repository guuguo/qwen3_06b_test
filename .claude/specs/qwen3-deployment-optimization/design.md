# Qwen-3 æ¨¡å‹éƒ¨ç½²ä¸ä¼˜åŒ–è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

æœ¬è®¾è®¡æ–‡æ¡£åŸºäºéœ€æ±‚æ–‡æ¡£ï¼Œè¯¦ç»†æè¿°äº† Qwen-3 ç³»åˆ—æ¨¡å‹ï¼ˆ0.6b å’Œ 1.7bï¼‰çš„å®Œæ•´éƒ¨ç½²ã€æ€§èƒ½æµ‹è¯•å’Œå¾®è°ƒè§£å†³æ–¹æ¡ˆçš„æŠ€æœ¯æ¶æ„ã€‚ç³»ç»Ÿå°†æ”¯æŒæœ¬åœ°éƒ¨ç½²ã€æœåŠ¡å™¨éƒ¨ç½²ã€Kubernetes å®¹å™¨åŒ–éƒ¨ç½²ï¼Œä»¥åŠæ—  GPU ç¯å¢ƒä¸‹çš„æ€§èƒ½ä¼˜åŒ–å’ŒåŸºäºé€šè¯è¯­ä¹‰çš„æŠ•è¯‰åˆ†ç±»å¾®è°ƒã€‚

## æ¶æ„

### æ•´ä½“æ¶æ„å›¾

```mermaid
graph TB
    subgraph "éƒ¨ç½²ç¯å¢ƒ"
        A[æœ¬åœ°å¼€å‘ç¯å¢ƒ] --> D[éƒ¨ç½²ç®¡ç†å™¨]
        B[Linux æœåŠ¡å™¨] --> D
        C[Kubernetes é›†ç¾¤] --> D
    end
    
    subgraph "æ ¸å¿ƒç»„ä»¶"
        D --> E[æ¨¡å‹æœåŠ¡å±‚]
        E --> F[æ¨ç†å¼•æ“]
        E --> G[æ€§èƒ½ç›‘æ§]
        E --> H[å¾®è°ƒç®¡ç†å™¨]
    end
    
    subgraph "æ”¯æŒæœåŠ¡"
        I[é…ç½®ç®¡ç†]
        J[å¥åº·æ£€æŸ¥]
        K[æ—¥å¿—æ”¶é›†]
        L[æŒ‡æ ‡ç›‘æ§]
    end
    
    F --> M[Qwen-3 0.6b/1.7b æ¨¡å‹]
    G --> N[æ€§èƒ½æµ‹è¯•å¥—ä»¶]
    H --> O[å¾®è°ƒè®­ç»ƒæµæ°´çº¿]
    
    E --> I
    E --> J
    E --> K
    E --> L
```

### æŠ€æœ¯æ ˆé€‰æ‹©

#### æœ¬åœ°å¼€å‘ç¯å¢ƒï¼ˆç®€åŒ–æ–¹æ¡ˆï¼‰
- **æ¨ç†å¼•æ“**: Ollamaï¼ˆå·²éƒ¨ç½²ï¼‰+ LMDeployï¼ˆå¯é€‰ä¼˜åŒ–ï¼‰
- **å¯è§‚æµ‹æ€§**: ç®€åŒ–æ—¥å¿— + åŸºç¡€æ€§èƒ½ç›‘æ§
- **æ€§èƒ½æµ‹è¯•**: è½»é‡çº§ Python è„šæœ¬
- **å¾®è°ƒæ¡†æ¶**: Transformers + LoRA
- **ç›‘æ§**: æ–‡ä»¶æ—¥å¿— + ç®€å• Web ç•Œé¢

#### ç”Ÿäº§ç¯å¢ƒï¼ˆå®Œæ•´æ–¹æ¡ˆï¼‰
- **æ¨ç†å¼•æ“**: LMDeployï¼ˆæ”¯æŒ CPU ä¼˜åŒ–ã€é‡åŒ–ã€KV ç¼“å­˜ï¼‰
- **å®¹å™¨åŒ–**: Docker + Kubernetes
- **æœåŠ¡æ¡†æ¶**: FastAPIï¼ˆRESTful APIï¼‰
- **æ€§èƒ½æµ‹è¯•**: Locust + è‡ªå®šä¹‰æ€§èƒ½åˆ†æå·¥å…·
- **å¾®è°ƒæ¡†æ¶**: Transformers + LoRA/QLoRA
- **ç›‘æ§**: Prometheus + Grafana
- **åŒ…ç®¡ç†**: Helm Charts

## ç»„ä»¶å’Œæ¥å£

### 1. éƒ¨ç½²ç®¡ç†å™¨ (Deployment Manager)

#### 1.1 æœ¬åœ°éƒ¨ç½²ç»„ä»¶ï¼ˆç®€åŒ–æ–¹æ¡ˆï¼‰

##### Ollama é›†æˆç»„ä»¶
```python
class OllamaIntegration:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.session = requests.Session()
        
    def check_ollama_status(self) -> bool:
        """æ£€æŸ¥ Ollama æœåŠ¡çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.base_url}/api/tags")
            return response.status_code == 200
        except:
            return False
            
    def list_models(self) -> List[str]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        
    def inference_with_metrics(self, model: str, prompt: str) -> Dict:
        """å¸¦æŒ‡æ ‡æ”¶é›†çš„æ¨ç†"""
        start_time = time.time()
        try:
            response = self.session.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                }
            )
            end_time = time.time()
            
            result = {
                "response": response.json().get("response", ""),
                "latency_ms": (end_time - start_time) * 1000,
                "status_code": response.status_code,
                "timestamp": datetime.now().isoformat()
            }
            
            # ç®€å•æ—¥å¿—è®°å½•
            self._log_metrics(result)
            return result
            
        except Exception as e:
            return {"error": str(e), "timestamp": datetime.now().isoformat()}
            
    def _log_metrics(self, metrics: Dict):
        """è®°å½•æŒ‡æ ‡åˆ°æ–‡ä»¶"""
        log_file = "ollama_metrics.jsonl"
        with open(log_file, "a") as f:
            f.write(json.dumps(metrics, ensure_ascii=False) + "\n")
```

##### ç®€åŒ–æ€§èƒ½ç›‘æ§ç»„ä»¶
```python
class SimplePerformanceMonitor:
    def __init__(self, log_file: str = "performance.log"):
        self.log_file = log_file
        self.metrics = []
        
    def start_monitoring(self):
        """å¼€å§‹ç›‘æ§ç³»ç»ŸæŒ‡æ ‡"""
        
    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        import psutil
        return {
            "cpu_percent": psutil.cpu_percent(),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_mb": psutil.virtual_memory().used / 1024 / 1024,
            "timestamp": datetime.now().isoformat()
        }
        
    def log_performance(self, metrics: Dict):
        """è®°å½•æ€§èƒ½æ•°æ®"""
        with open(self.log_file, "a") as f:
            f.write(f"{datetime.now()}: {json.dumps(metrics)}\n")
            
    def generate_simple_report(self) -> str:
        """ç”Ÿæˆç®€å•æŠ¥å‘Š"""
        # è¯»å–æœ€è¿‘çš„æŒ‡æ ‡æ•°æ®å¹¶ç”ŸæˆæŠ¥å‘Š
        pass
```

#### 1.2 æœ¬åœ°éƒ¨ç½²ç»„ä»¶ï¼ˆå®Œæ•´æ–¹æ¡ˆï¼‰
```python
class LocalDeployment:
    def __init__(self, model_path: str, config: Dict):
        self.model_path = model_path
        self.config = config
        
    def setup_environment(self) -> bool:
        """è®¾ç½®æœ¬åœ°ç¯å¢ƒä¾èµ–"""
        
    def deploy_model(self) -> bool:
        """éƒ¨ç½²æ¨¡å‹åˆ°æœ¬åœ°ç¯å¢ƒ"""
        
    def validate_deployment(self) -> bool:
        """éªŒè¯éƒ¨ç½²çŠ¶æ€"""
```

**æ¥å£è®¾è®¡**:
- `POST /api/v1/local/deploy` - å¯åŠ¨æœ¬åœ°éƒ¨ç½²
- `GET /api/v1/local/status` - æ£€æŸ¥éƒ¨ç½²çŠ¶æ€
- `DELETE /api/v1/local/cleanup` - æ¸…ç†éƒ¨ç½²

#### 1.2 æœåŠ¡å™¨éƒ¨ç½²ç»„ä»¶
```python
class ServerDeployment:
    def __init__(self, server_config: Dict):
        self.server_config = server_config
        
    def deploy_service(self) -> bool:
        """éƒ¨ç½²æœåŠ¡åˆ° Linux æœåŠ¡å™¨"""
        
    def configure_systemd(self) -> bool:
        """é…ç½® systemd æœåŠ¡"""
        
    def setup_load_balancer(self) -> bool:
        """é…ç½®è´Ÿè½½å‡è¡¡"""
```

#### 1.3 Kubernetes éƒ¨ç½²ç»„ä»¶
```python
class KubernetesDeployment:
    def __init__(self, k8s_config: Dict):
        self.k8s_config = k8s_config
        
    def build_docker_image(self) -> str:
        """æ„å»º Docker é•œåƒ"""
        
    def deploy_to_k8s(self) -> bool:
        """éƒ¨ç½²åˆ° Kubernetes é›†ç¾¤"""
        
    def setup_hpa(self) -> bool:
        """é…ç½®æ°´å¹³è‡ªåŠ¨æ‰©ç¼©å®¹"""
```

**Kubernetes èµ„æºé…ç½®**:
```yaml
# Deployment é…ç½®
apiVersion: apps/v1
kind: Deployment
metadata:
  name: qwen3-inference
spec:
  replicas: 3
  selector:
    matchLabels:
      app: qwen3-inference
  template:
    spec:
      containers:
      - name: qwen3-server
        image: qwen3-inference:latest
        resources:
          requests:
            memory: "4Gi"
            cpu: "2"
          limits:
            memory: "8Gi"
            cpu: "4"
        readinessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
```

### 2. æ¨¡å‹æœåŠ¡å±‚ (Model Service Layer)

#### 2.1 æ¨ç†æœåŠ¡
```python
class InferenceService:
    def __init__(self, model_name: str, optimization_config: Dict):
        self.model_name = model_name
        self.optimization_config = optimization_config
        self.engine = LMDeployEngine(model_name, optimization_config)
        
    async def predict(self, input_text: str) -> Dict:
        """æ¨¡å‹æ¨ç†æ¥å£"""
        
    def get_model_info(self) -> Dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
```

**API æ¥å£è®¾è®¡**:
```yaml
openapi: 3.0.0
paths:
  /api/v1/inference:
    post:
      summary: æ¨¡å‹æ¨ç†
      requestBody:
        content:
          application/json:
            schema:
              type: object
              properties:
                text:
                  type: string
                max_tokens:
                  type: integer
                temperature:
                  type: number
      responses:
        200:
          description: æ¨ç†ç»“æœ
          content:
            application/json:
              schema:
                type: object
                properties:
                  result:
                    type: string
                  tokens_used:
                    type: integer
                  response_time:
                    type: number
```

#### 2.2 ä¼˜åŒ–å¼•æ“
```python
class OptimizationEngine:
    def __init__(self):
        self.quantization_config = {}
        self.kv_cache_config = {}
        
    def apply_quantization(self, model_path: str) -> str:
        """åº”ç”¨æ¨¡å‹é‡åŒ–"""
        
    def optimize_kv_cache(self, config: Dict) -> Dict:
        """ä¼˜åŒ– KV ç¼“å­˜é…ç½®"""
        
    def tune_cpu_inference(self) -> Dict:
        """CPU æ¨ç†ä¼˜åŒ–"""
```

### 3. æ€§èƒ½æµ‹è¯•å¥—ä»¶ (Performance Testing Suite)

#### 3.1 ç®€åŒ–æœ¬åœ°æµ‹è¯•ç»„ä»¶
```python
class SimpleLocalTester:
    def __init__(self, ollama_integration: OllamaIntegration):
        self.ollama = ollama_integration
        self.results = []
        
    def run_basic_qps_test(self, model: str, test_prompts: List[str], 
                          concurrent_users: int = 5, duration: int = 60) -> Dict:
        """è¿è¡ŒåŸºç¡€ QPS æµ‹è¯•"""
        import threading
        import time
        from concurrent.futures import ThreadPoolExecutor
        
        results = []
        start_time = time.time()
        end_time = start_time + duration
        
        def worker():
            while time.time() < end_time:
                for prompt in test_prompts:
                    result = self.ollama.inference_with_metrics(model, prompt)
                    results.append(result)
                    
        with ThreadPoolExecutor(max_workers=concurrent_users) as executor:
            futures = [executor.submit(worker) for _ in range(concurrent_users)]
            
        # è®¡ç®—æŒ‡æ ‡
        total_requests = len(results)
        successful_requests = len([r for r in results if 'error' not in r])
        avg_latency = sum(r.get('latency_ms', 0) for r in results) / len(results)
        qps = total_requests / duration
        
        summary = {
            "total_requests": total_requests,
            "successful_requests": successful_requests,
            "error_rate": (total_requests - successful_requests) / total_requests,
            "avg_latency_ms": avg_latency,
            "qps": qps,
            "test_duration": duration,
            "concurrent_users": concurrent_users
        }
        
        self._save_test_results(summary, results)
        return summary
        
    def run_latency_test(self, model: str, test_prompts: List[str], 
                        iterations: int = 100) -> Dict:
        """è¿è¡Œå»¶è¿Ÿæµ‹è¯•"""
        latencies = []
        
        for i in range(iterations):
            prompt = test_prompts[i % len(test_prompts)]
            result = self.ollama.inference_with_metrics(model, prompt)
            if 'latency_ms' in result:
                latencies.append(result['latency_ms'])
                
        latencies.sort()
        summary = {
            "iterations": iterations,
            "avg_latency_ms": sum(latencies) / len(latencies),
            "min_latency_ms": min(latencies),
            "max_latency_ms": max(latencies),
            "p50_latency_ms": latencies[len(latencies)//2],
            "p95_latency_ms": latencies[int(len(latencies)*0.95)],
            "p99_latency_ms": latencies[int(len(latencies)*0.99)]
        }
        
        return summary
        
    def _save_test_results(self, summary: Dict, detailed_results: List[Dict]):
        """ä¿å­˜æµ‹è¯•ç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜æ‘˜è¦
        with open(f"test_summary_{timestamp}.json", "w") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
            
        # ä¿å­˜è¯¦ç»†ç»“æœ
        with open(f"test_details_{timestamp}.jsonl", "w") as f:
            for result in detailed_results:
                f.write(json.dumps(result, ensure_ascii=False) + "\n")
                
    def generate_simple_html_report(self, summary: Dict) -> str:
        """ç”Ÿæˆç®€å•çš„ HTML æŠ¥å‘Š"""
        html_template = """
        <!DOCTYPE html>
        <html>
        <head>
            <title>Qwen-3 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                .metric {{ margin: 10px 0; padding: 10px; background: #f5f5f5; }}
                .good {{ color: green; }}
                .warning {{ color: orange; }}
                .error {{ color: red; }}
            </style>
        </head>
        <body>
            <h1>Qwen-3 æ€§èƒ½æµ‹è¯•æŠ¥å‘Š</h1>
            <div class="metric">QPS: {qps:.2f}</div>
            <div class="metric">å¹³å‡å»¶è¿Ÿ: {avg_latency_ms:.2f}ms</div>
            <div class="metric">é”™è¯¯ç‡: {error_rate:.2%}</div>
            <div class="metric">æˆåŠŸè¯·æ±‚: {successful_requests}/{total_requests}</div>
            <div class="metric">å¹¶å‘ç”¨æˆ·: {concurrent_users}</div>
            <div class="metric">æµ‹è¯•æ—¶é•¿: {test_duration}s</div>
        </body>
        </html>
        """.format(**summary)
        
        report_file = f"performance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html"
        with open(report_file, "w", encoding="utf-8") as f:
            f.write(html_template)
            
        return report_file
```

#### 3.2 æ€§èƒ½æµ‹è¯•ç®¡ç†å™¨ï¼ˆå®Œæ•´æ–¹æ¡ˆï¼‰
```python
class PerformanceTestManager:
    def __init__(self, test_config: Dict):
        self.test_config = test_config
        self.metrics_collector = MetricsCollector()
        
    def run_qps_test(self, concurrent_users: int, duration: int) -> Dict:
        """QPS æ€§èƒ½æµ‹è¯•"""
        
    def run_latency_test(self, request_count: int) -> Dict:
        """å»¶è¿Ÿæµ‹è¯•"""
        
    def run_stress_test(self, max_concurrent: int) -> Dict:
        """å‹åŠ›æµ‹è¯•"""
        
    def generate_report(self, test_results: List[Dict]) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
```

#### 3.2 æŒ‡æ ‡æ”¶é›†å™¨
```python
class MetricsCollector:
    def collect_system_metrics(self) -> Dict:
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡ï¼ˆCPUã€å†…å­˜ã€ç£ç›˜ï¼‰"""
        
    def collect_inference_metrics(self) -> Dict:
        """æ”¶é›†æ¨ç†æŒ‡æ ‡ï¼ˆQPSã€RTã€ååé‡ï¼‰"""
        
    def collect_model_metrics(self) -> Dict:
        """æ”¶é›†æ¨¡å‹æŒ‡æ ‡ï¼ˆå†…å­˜ä½¿ç”¨ã€åŠ è½½æ—¶é—´ï¼‰"""
```

### 4. å¾®è°ƒç®¡ç†å™¨ (Fine-tuning Manager)

#### 4.1 æ•°æ®é¢„å¤„ç†å™¨
```python
class DataPreprocessor:
    def __init__(self, data_config: Dict):
        self.data_config = data_config
        
    def preprocess_call_semantics(self, raw_data: List[Dict]) -> List[Dict]:
        """é¢„å¤„ç†é€šè¯è¯­ä¹‰æ•°æ®"""
        
    def create_complaint_labels(self, data: List[Dict]) -> Dict:
        """åˆ›å»ºæŠ•è¯‰åˆ†ç±»æ ‡ç­¾"""
        
    def split_dataset(self, data: List[Dict]) -> Tuple[List, List, List]:
        """åˆ†å‰²è®­ç»ƒã€éªŒè¯ã€æµ‹è¯•æ•°æ®é›†"""
```

#### 4.2 å¾®è°ƒè®­ç»ƒå™¨
```python
class FineTuningTrainer:
    def __init__(self, model_config: Dict, training_config: Dict):
        self.model_config = model_config
        self.training_config = training_config
        
    def setup_lora_config(self) -> Dict:
        """é…ç½® LoRA å‚æ•°"""
        
    def train_model(self, train_data: List[Dict]) -> str:
        """æ‰§è¡Œå¾®è°ƒè®­ç»ƒ"""
        
    def evaluate_model(self, test_data: List[Dict]) -> Dict:
        """è¯„ä¼°å¾®è°ƒæ¨¡å‹"""
        
    def save_checkpoint(self, epoch: int) -> str:
        """ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹"""
```

## æ•°æ®æ¨¡å‹

### 1. é…ç½®æ•°æ®æ¨¡å‹

```python
@dataclass
class DeploymentConfig:
    model_name: str
    model_version: str
    deployment_type: str  # local, server, kubernetes
    resource_limits: ResourceLimits
    optimization_settings: OptimizationSettings

@dataclass
class ResourceLimits:
    cpu_cores: int
    memory_gb: int
    disk_gb: int
    gpu_count: int = 0

@dataclass
class OptimizationSettings:
    enable_quantization: bool
    quantization_bits: int
    enable_kv_cache: bool
    max_batch_size: int
    max_sequence_length: int
```

### 2. æ€§èƒ½æµ‹è¯•æ•°æ®æ¨¡å‹

```python
@dataclass
class PerformanceMetrics:
    timestamp: datetime
    qps: float
    avg_latency_ms: float
    p95_latency_ms: float
    p99_latency_ms: float
    cpu_usage_percent: float
    memory_usage_mb: float
    error_rate_percent: float

@dataclass
class TestResult:
    test_id: str
    test_type: str
    model_version: str
    configuration: Dict
    metrics: List[PerformanceMetrics]
    summary: Dict
```

### 3. å¾®è°ƒæ•°æ®æ¨¡å‹

```python
@dataclass
class TrainingData:
    conversation_id: str
    input_text: str
    complaint_category: str
    confidence_score: float
    metadata: Dict

@dataclass
class FineTuningResult:
    model_id: str
    base_model: str
    training_config: Dict
    evaluation_metrics: Dict
    model_path: str
    created_at: datetime
```

## é”™è¯¯å¤„ç†

### 1. éƒ¨ç½²é”™è¯¯å¤„ç†

```python
class DeploymentError(Exception):
    """éƒ¨ç½²ç›¸å…³é”™è¯¯åŸºç±»"""
    pass

class ModelLoadError(DeploymentError):
    """æ¨¡å‹åŠ è½½é”™è¯¯"""
    pass

class ResourceInsufficientError(DeploymentError):
    """èµ„æºä¸è¶³é”™è¯¯"""
    pass

class ConfigurationError(DeploymentError):
    """é…ç½®é”™è¯¯"""
    pass
```

### 2. é”™è¯¯æ¢å¤ç­–ç•¥

- **æ¨¡å‹åŠ è½½å¤±è´¥**: è‡ªåŠ¨é‡è¯• 3 æ¬¡ï¼Œé™çº§åˆ°å¤‡ç”¨æ¨¡å‹
- **å†…å­˜ä¸è¶³**: è‡ªåŠ¨è°ƒæ•´æ‰¹å¤„ç†å¤§å°ï¼Œå¯ç”¨å†…å­˜ä¼˜åŒ–
- **ç½‘ç»œè¿æ¥å¤±è´¥**: æŒ‡æ•°é€€é¿é‡è¯•ï¼Œæœ€å¤§é‡è¯• 5 æ¬¡
- **Kubernetes Pod å¤±è´¥**: è‡ªåŠ¨é‡å¯ï¼Œå¥åº·æ£€æŸ¥å¤±è´¥æ—¶æ›¿æ¢ Pod

### 3. ç›‘æ§å’Œå‘Šè­¦

```python
class AlertManager:
    def setup_alerts(self):
        """é…ç½®å‘Šè­¦è§„åˆ™"""
        alerts = [
            Alert("high_latency", "avg_latency > 1000ms", "warning"),
            Alert("low_qps", "qps < 10", "warning"),
            Alert("high_error_rate", "error_rate > 5%", "critical"),
            Alert("memory_usage", "memory_usage > 80%", "warning"),
            Alert("pod_restart", "pod_restart_count > 3", "critical")
        ]
        return alerts
```

## æµ‹è¯•ç­–ç•¥

### 1. å•å…ƒæµ‹è¯•
- æ¨¡å‹åŠ è½½å’Œæ¨ç†åŠŸèƒ½æµ‹è¯•
- é…ç½®è§£æå’ŒéªŒè¯æµ‹è¯•
- æ•°æ®é¢„å¤„ç†åŠŸèƒ½æµ‹è¯•
- API æ¥å£æµ‹è¯•

### 2. é›†æˆæµ‹è¯•
- ç«¯åˆ°ç«¯éƒ¨ç½²æµç¨‹æµ‹è¯•
- æ€§èƒ½æµ‹è¯•æµæ°´çº¿æµ‹è¯•
- å¾®è°ƒè®­ç»ƒæµç¨‹æµ‹è¯•
- Kubernetes éƒ¨ç½²æµ‹è¯•

### 3. æ€§èƒ½æµ‹è¯•
- åŸºå‡†æ€§èƒ½æµ‹è¯•ï¼ˆQPSã€å»¶è¿Ÿï¼‰
- å‹åŠ›æµ‹è¯•ï¼ˆæœ€å¤§å¹¶å‘ã€èµ„æºé™åˆ¶ï¼‰
- ç¨³å®šæ€§æµ‹è¯•ï¼ˆé•¿æ—¶é—´è¿è¡Œï¼‰
- ä¼˜åŒ–æ•ˆæœå¯¹æ¯”æµ‹è¯•

### 4. å®‰å…¨æµ‹è¯•
- API å®‰å…¨æµ‹è¯•
- å®¹å™¨å®‰å…¨æ‰«æ
- é…ç½®å®‰å…¨æ£€æŸ¥
- æ•°æ®éšç§ä¿æŠ¤æµ‹è¯•

## éƒ¨ç½²ç­–ç•¥

### 1. æœ¬åœ°éƒ¨ç½²ç­–ç•¥
- ä½¿ç”¨ Python è™šæ‹Ÿç¯å¢ƒéš”ç¦»ä¾èµ–
- è‡ªåŠ¨æ£€æµ‹å’Œå®‰è£…ç³»ç»Ÿä¾èµ–
- æä¾›ä¸€é”®éƒ¨ç½²è„šæœ¬
- æ”¯æŒå¼€å‘æ¨¡å¼å’Œç”Ÿäº§æ¨¡å¼

### 2. æœåŠ¡å™¨éƒ¨ç½²ç­–ç•¥
- ä½¿ç”¨ systemd ç®¡ç†æœåŠ¡ç”Ÿå‘½å‘¨æœŸ
- é…ç½®æ—¥å¿—è½®è½¬å’Œç›‘æ§
- æ”¯æŒå¤šå®ä¾‹è´Ÿè½½å‡è¡¡
- æä¾›æ»šåŠ¨æ›´æ–°æœºåˆ¶

### 3. Kubernetes éƒ¨ç½²ç­–ç•¥
- ä½¿ç”¨ Helm Charts ç®¡ç†éƒ¨ç½²
- é…ç½® HPA è‡ªåŠ¨æ‰©ç¼©å®¹
- å®ç°è“ç»¿éƒ¨ç½²å’Œé‡‘ä¸é›€å‘å¸ƒ
- é›†æˆæœåŠ¡ç½‘æ ¼ï¼ˆå¯é€‰ï¼‰

### 4. é…ç½®ç®¡ç†ç­–ç•¥
- ä½¿ç”¨ ConfigMap ç®¡ç†åº”ç”¨é…ç½®
- ä½¿ç”¨ Secret ç®¡ç†æ•æ„Ÿä¿¡æ¯
- æ”¯æŒç¯å¢ƒå˜é‡è¦†ç›–
- æä¾›é…ç½®çƒ­æ›´æ–°æœºåˆ¶

## ç›‘æ§å’Œå¯è§‚æµ‹æ€§

### 0. æœ¬åœ°ç®€åŒ–ç›‘æ§æ–¹æ¡ˆ

#### ç®€å•æ–‡ä»¶æ—¥å¿—ç›‘æ§
```python
class SimpleFileMonitor:
    def __init__(self, log_dir: str = "./logs"):
        self.log_dir = log_dir
        os.makedirs(log_dir, exist_ok=True)
        
    def log_request(self, model: str, prompt: str, response: str, 
                   latency_ms: float, error: str = None):
        """è®°å½•è¯·æ±‚æ—¥å¿—"""
        log_entry = {
            "timestamp": datetime.now().isoformat(),
            "model": model,
            "prompt_length": len(prompt),
            "response_length": len(response) if response else 0,
            "latency_ms": latency_ms,
            "error": error,
            "status": "error" if error else "success"
        }
        
        log_file = os.path.join(self.log_dir, f"requests_{datetime.now().strftime('%Y%m%d')}.jsonl")
        with open(log_file, "a") as f:
            f.write(json.dumps(log_entry, ensure_ascii=False) + "\n")
            
    def log_system_metrics(self):
        """è®°å½•ç³»ç»ŸæŒ‡æ ‡"""
        import psutil
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "memory_used_gb": psutil.virtual_memory().used / 1024**3,
            "disk_usage_percent": psutil.disk_usage('/').percent
        }
        
        log_file = os.path.join(self.log_dir, f"system_{datetime.now().strftime('%Y%m%d')}.jsonl")
        with open(log_file, "a") as f:
            f.write(json.dumps(metrics) + "\n")
            
    def generate_daily_summary(self, date: str = None) -> Dict:
        """ç”Ÿæˆæ—¥åº¦æ‘˜è¦æŠ¥å‘Š"""
        if not date:
            date = datetime.now().strftime('%Y%m%d')
            
        request_file = os.path.join(self.log_dir, f"requests_{date}.jsonl")
        system_file = os.path.join(self.log_dir, f"system_{date}.jsonl")
        
        summary = {
            "date": date,
            "total_requests": 0,
            "successful_requests": 0,
            "error_requests": 0,
            "avg_latency_ms": 0,
            "max_latency_ms": 0,
            "avg_cpu_percent": 0,
            "max_memory_percent": 0
        }
        
        # åˆ†æè¯·æ±‚æ—¥å¿—
        if os.path.exists(request_file):
            latencies = []
            with open(request_file, 'r') as f:
                for line in f:
                    entry = json.loads(line)
                    summary["total_requests"] += 1
                    if entry["status"] == "success":
                        summary["successful_requests"] += 1
                        latencies.append(entry["latency_ms"])
                    else:
                        summary["error_requests"] += 1
                        
            if latencies:
                summary["avg_latency_ms"] = sum(latencies) / len(latencies)
                summary["max_latency_ms"] = max(latencies)
                
        # åˆ†æç³»ç»ŸæŒ‡æ ‡
        if os.path.exists(system_file):
            cpu_values = []
            memory_values = []
            with open(system_file, 'r') as f:
                for line in f:
                    entry = json.loads(line)
                    cpu_values.append(entry["cpu_percent"])
                    memory_values.append(entry["memory_percent"])
                    
            if cpu_values:
                summary["avg_cpu_percent"] = sum(cpu_values) / len(cpu_values)
            if memory_values:
                summary["max_memory_percent"] = max(memory_values)
                
        return summary
```

#### ç®€å• Web ä»ªè¡¨ç›˜
```python
from flask import Flask, render_template, jsonify

class SimpleWebDashboard:
    def __init__(self, monitor: SimpleFileMonitor, port: int = 5000):
        self.monitor = monitor
        self.app = Flask(__name__)
        self.port = port
        self._setup_routes()
        
    def _setup_routes(self):
        @self.app.route('/')
        def dashboard():
            return render_template('dashboard.html')
            
        @self.app.route('/api/summary')
        def get_summary():
            summary = self.monitor.generate_daily_summary()
            return jsonify(summary)
            
        @self.app.route('/api/recent_requests')
        def get_recent_requests():
            # è¿”å›æœ€è¿‘ 100 æ¡è¯·æ±‚è®°å½•
            return jsonify(self._get_recent_logs(100))
            
    def _get_recent_logs(self, limit: int = 100) -> List[Dict]:
        """è·å–æœ€è¿‘çš„æ—¥å¿—è®°å½•"""
        today = datetime.now().strftime('%Y%m%d')
        log_file = os.path.join(self.monitor.log_dir, f"requests_{today}.jsonl")
        
        logs = []
        if os.path.exists(log_file):
            with open(log_file, 'r') as f:
                lines = f.readlines()
                for line in lines[-limit:]:
                    logs.append(json.loads(line))
        return logs
        
    def run(self, debug: bool = True):
        """å¯åŠ¨ Web ä»ªè¡¨ç›˜"""
        self.app.run(host='0.0.0.0', port=self.port, debug=debug)
```

#### ä¸€é”®å¯åŠ¨è„šæœ¬
```python
# simple_monitor.py - ä¸€é”®å¯åŠ¨æœ¬åœ°ç›‘æ§
class LocalMonitoringSetup:
    def __init__(self):
        self.ollama = OllamaIntegration()
        self.monitor = SimpleFileMonitor()
        self.tester = SimpleLocalTester(self.ollama)
        
    def start_monitoring(self):
        """å¯åŠ¨ç›‘æ§"""
        print("ğŸš€ å¯åŠ¨ Qwen-3 æœ¬åœ°ç›‘æ§...")
        
        # æ£€æŸ¥ Ollama çŠ¶æ€
        if not self.ollama.check_ollama_status():
            print("âŒ Ollama æœåŠ¡æœªè¿è¡Œï¼Œè¯·å…ˆå¯åŠ¨ Ollama")
            return
            
        print("âœ… Ollama æœåŠ¡æ­£å¸¸")
        
        # å¯åŠ¨ç³»ç»ŸæŒ‡æ ‡æ”¶é›†
        import threading
        def collect_metrics():
            while True:
                self.monitor.log_system_metrics()
                time.sleep(60)  # æ¯åˆ†é’Ÿæ”¶é›†ä¸€æ¬¡
                
        metrics_thread = threading.Thread(target=collect_metrics, daemon=True)
        metrics_thread.start()
        
        print("âœ… ç³»ç»ŸæŒ‡æ ‡æ”¶é›†å·²å¯åŠ¨")
        
        # å¯åŠ¨ Web ä»ªè¡¨ç›˜
        dashboard = SimpleWebDashboard(self.monitor)
        print("âœ… Web ä»ªè¡¨ç›˜å·²å¯åŠ¨: http://localhost:5000")
        
        dashboard.run()
        
    def run_quick_test(self, model: str = "qwen3:0.6b"):
        """è¿è¡Œå¿«é€Ÿæµ‹è¯•"""
        print(f"ğŸ“Š å¼€å§‹å¯¹ {model} è¿›è¡Œå¿«é€Ÿæ€§èƒ½æµ‹è¯•...")
        
        test_prompts = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚",
            "è¯·è§£é‡Šä¸€ä¸‹äººå·¥æ™ºèƒ½çš„åŸºæœ¬æ¦‚å¿µã€‚",
            "å†™ä¸€ä¸ªç®€å•çš„ Python å‡½æ•°æ¥è®¡ç®—é˜¶ä¹˜ã€‚"
        ]
        
        # è¿è¡Œå»¶è¿Ÿæµ‹è¯•
        latency_result = self.tester.run_latency_test(model, test_prompts, 10)
        print(f"âœ… å»¶è¿Ÿæµ‹è¯•å®Œæˆ: å¹³å‡ {latency_result['avg_latency_ms']:.2f}ms")
        
        # è¿è¡Œç®€å• QPS æµ‹è¯•
        qps_result = self.tester.run_basic_qps_test(model, test_prompts, 2, 30)
        print(f"âœ… QPS æµ‹è¯•å®Œæˆ: {qps_result['qps']:.2f} QPS")
        
        # ç”ŸæˆæŠ¥å‘Š
        report_file = self.tester.generate_simple_html_report(qps_result)
        print(f"âœ… æ€§èƒ½æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
        
if __name__ == "__main__":
    setup = LocalMonitoringSetup()
    
    import sys
    if len(sys.argv) > 1 and sys.argv[1] == "test":
        setup.run_quick_test()
    else:
        setup.start_monitoring()
```

### 1. æŒ‡æ ‡ç›‘æ§ï¼ˆç”Ÿäº§ç¯å¢ƒï¼‰
```yaml
# Prometheus ç›‘æ§æŒ‡æ ‡
metrics:
  - name: qwen3_inference_requests_total
    type: counter
    help: "Total number of inference requests"
    
  - name: qwen3_inference_duration_seconds
    type: histogram
    help: "Inference request duration"
    
  - name: qwen3_model_memory_usage_bytes
    type: gauge
    help: "Model memory usage in bytes"
    
  - name: qwen3_active_connections
    type: gauge
    help: "Number of active connections"
```

### 2. æ—¥å¿—ç®¡ç†
- ç»“æ„åŒ–æ—¥å¿—è¾“å‡ºï¼ˆJSON æ ¼å¼ï¼‰
- æ—¥å¿—çº§åˆ«é…ç½®ï¼ˆDEBUGã€INFOã€WARNINGã€ERRORï¼‰
- é›†ä¸­åŒ–æ—¥å¿—æ”¶é›†ï¼ˆELK Stack æˆ– Lokiï¼‰
- æ—¥å¿—è½®è½¬å’Œå½’æ¡£ç­–ç•¥

### 3. é“¾è·¯è¿½è¸ª
- ä½¿ç”¨ OpenTelemetry è¿›è¡Œåˆ†å¸ƒå¼è¿½è¸ª
- è¿½è¸ªè¯·æ±‚åœ¨å„ç»„ä»¶é—´çš„æµè½¬
- æ€§èƒ½ç“¶é¢ˆè¯†åˆ«å’Œåˆ†æ
- é”™è¯¯ä¼ æ’­è·¯å¾„è¿½è¸ª

è¿™ä¸ªè®¾è®¡æ–‡æ¡£æä¾›äº†å®Œæ•´çš„æŠ€æœ¯æ¶æ„ï¼Œæ¶µç›–äº†æ‰€æœ‰éœ€æ±‚æ–‡æ¡£ä¸­æåˆ°çš„åŠŸèƒ½ç‚¹ï¼Œå¹¶è€ƒè™‘äº†å¯æ‰©å±•æ€§ã€å¯ç»´æŠ¤æ€§å’Œç”Ÿäº§ç¯å¢ƒçš„å®é™…éœ€æ±‚ã€‚
